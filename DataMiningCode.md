# conda

## å®‰è£…ç¯å¢ƒ

```shell
conda create -n TF2.1 python=3.7

conda activate TF2.1

conda install cudatoolkit=10.1

conda install cudnn=7.6

pip install tensorflow==2.1

===éªŒè¯è¿›å…¥pythonç¯å¢ƒ===
>>>import tensorflow as tf
>>>tf.__version__
```



## æŸ¥è¯¢

```shell
conda env list

# æ¿€æ´»env_name
conda activate [env_name]

# è¿›å…¥jupyter lab
jupyter lab 
```





# å¸¸ç”¨æ•°æ®é›†

## IRIS







## MNIST

![image-20210913184459039](img/image-20210913184459039.png)



- å¯¼å…¥æ•°æ®

  ```python
  import tensorflow as tf
  
  mnist = tf.keras.datasets.mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  ```



- æ•°æ®å¯è§†åŒ–
    ```python
    import tensorflow as tf
    from matplotlib import pyplot as plt
    
    # å¯¼å…¥æ•°æ®é›†
    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # ==================å¯è§†åŒ–======================
    # å¯è§†åŒ–è®­ç»ƒé›†è¾“å…¥ç‰¹å¾çš„ç¬¬ä¸€ä¸ªå›¾ç‰‡
    plt.imshow(x_train[0], cmap='gray')  # ç»˜åˆ¶ç°åº¦å›¾
    plt.show()
    
    # æ‰“å°å‡ºè®­ç»ƒé›†è¾“å…¥ç‰¹å¾çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
    print("x_train[0]:\n", x_train[0])
    
    # æ‰“å°å‡ºè®­ç»ƒé›†æ ‡ç­¾çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
    print("y_train[0]:\n", y_train[0])
    
    # æ‰“å°å‡ºæ•´ä¸ªè®­ç»ƒé›†è¾“å…¥ç‰¹å¾å½¢çŠ¶
    print("x_train.shape:\n", x_train.shape)
    
    # æ‰“å°å‡ºæ•´ä¸ªè®­ç»ƒé›†æ ‡ç­¾çš„å½¢çŠ¶
    print("y_train.shape:\n", y_train.shape)
    # ===============================================
    ```



## FASHION

![image-20210913185818337](img/image-20210913185818337.png)



- å¯¼å…¥æ•°æ®

  ```python
  import tensorflow as tf
  
  fashion = tf.keras.datasets.fashion_mnist
  (x_train, y_train),(x_test, y_test) = fashion.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  ```

  

## Cifar10

Cifar10æ•°æ®é›†ï¼š
æä¾›5ä¸‡å¼ 32*32 åƒç´ ç‚¹çš„ååˆ†ç±»å½©è‰²å›¾ç‰‡å’Œæ ‡ç­¾ï¼Œç”¨äºè®­ç»ƒã€‚
æä¾›1ä¸‡å¼ 32*32 åƒç´ ç‚¹çš„ååˆ†ç±»å½©è‰²å›¾ç‰‡å’Œæ ‡ç­¾ï¼Œç”¨äºæµ‹è¯•ã€‚

![image-20210923193232786](img/image-20210923193232786.png)



- å¯¼å…¥æ•°æ®

  ```python
  cifar10 = tf.keras.datasets.cifar10
  (x_train, y_train),(x_test, y_test) = cifar10.load_data()
  ```

  





# TensorFlow2.1

![image-20210910151051087](img/image-20210910151051087.png)



## å¸¸ç”¨æ–¹æ³•

### å¼ é‡

<img src="img/image-20210909150959178.png" alt="image-20210909150959178" style="zoom:67%;" />

![image-20210909151544109](img/image-20210909151544109.png)





- `tf.constant(å¼ é‡å†…å®¹ï¼Œdtype=æ•°æ®ç±»å‹(å¯é€‰))`

  ```python
  a = tf.constant([1, 5], dtype=tf.int64)
  print(a)
  print(a.dtype)
  print(a.shape)
  ```

  

- `tf.convert_to_tensor(æ•°æ®åï¼Œdtype=æ•°æ®ç±»å‹(å¯é€‰))`

  å°†numpyçš„æ•°æ®ç±»å‹è½¬æ¢ä¸ºTensoræ•°æ®ç±»å‹

  ```python
  import numpy as np
  a = np.arange(0, 5)
  b = tf.convert_to_tensor(a, dtype=tf.int64)
  print(a)
  print(b)
  ```

  

- `tf.zeros(ç»´åº¦)`

  åˆ›å»ºå…¨ä¸º0çš„å¼ é‡



- `tf.ones(ç»´åº¦)`

  åˆ›å»ºå…¨ä¸º1çš„å¼ é‡



- `tf.fill(ç»´åº¦ï¼ŒæŒ‡å®šå€¼)`

  åˆ›å»ºå…¨ä¸ºæŒ‡å®šå€¼çš„å¼ é‡



- ç»´åº¦ï¼š
  - ä¸€ç»´ç›´æ¥å†™ä¸ªæ•°
  - äºŒç»´ç”¨[è¡Œï¼Œåˆ—]
  - å¤šç»´ç”¨[n,m,j,kâ€¦â€¦]

```python
a = tf.zeros([2, 3])
b = tf.ones(4)
c = tf.fill([2, 2], 9)
print(a)
print(b)
print(c)
```



- np.random.RandomState.rand()
  è¿”å›ä¸€ä¸ª[0,1)ä¹‹é—´çš„éšæœºæ•°
  `np.random.RandomState.rand(ç»´åº¦)`

  ```python
  import numpy as np
  rdm=np.random.RandomState(seed=1) # seed=å¸¸æ•°æ¯æ¬¡ç”Ÿæˆéšæœºæ•°ç›¸åŒ
  a=rdm.rand()     # è¿”å›ä¸€ä¸ªéšæœºæ ‡é‡
  b=rdm.rand(2, 3) # è¿”å›ç»´åº¦ä¸º2è¡Œ3åˆ—éšæœºæ•°çŸ©é˜µ
  print("a:", a)
  print("b:", b)
  
  # a: 0.417022004702574
  # b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01]
  # 	  [1.46755891e-01 9.23385948e-02 1.86260211e-01]]
  ```

  

- `tf.random.normal(ç»´åº¦ï¼Œmean=å‡å€¼ï¼Œstddev=æ ‡å‡†å·®)`

  ç”Ÿæˆæ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œé»˜è®¤å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1



- `tf.random.truncated_normal (ç»´åº¦ï¼Œmean=å‡å€¼ï¼Œstddev=æ ‡å‡†å·®)`

  ç”Ÿæˆæˆªæ–­å¼æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œåœ¨`tf.truncated_normal`ä¸­å¦‚æœéšæœºç”Ÿæˆæ•°æ®çš„å–å€¼åœ¨ï¼ˆÎ¼-2Ïƒï¼ŒÎ¼+2Ïƒï¼‰ä¹‹å¤–åˆ™é‡æ–°è¿›è¡Œç”Ÿæˆï¼Œä¿è¯äº†ç”Ÿæˆå€¼åœ¨å‡å€¼é™„è¿‘

```python
d = tf.random.normal([2, 2], mean=5, stddev=1)
print(d)
e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=10)
print(e)
```



- `tf.random.uniform(ç»´åº¦ï¼Œminval=æœ€å°å€¼ï¼Œmaxval=æœ€å¤§å€¼)`

  ç”Ÿæˆå‡åŒ€åˆ†å¸ƒéšæœºæ•°[minval, maxval)

  ```python
  f = tf.random.uniform([5, 5], minval=0, maxval=1)
  print(f)
  ```



- `tf.cast(å¼ é‡åï¼Œdtype=æ•°æ®ç±»å‹)`

  å¼ºåˆ¶tensorè½¬æ¢ä¸ºè¯¥æ•°æ®ç±»å‹



- `tf.reduce_min(å¼ é‡å)`

  è®¡ç®—å¼ é‡ç»´åº¦ä¸Šå…ƒç´ çš„æœ€å°å€¼

- `tf.reduce_max(å¼ é‡å)`

  è®¡ç®—å¼ é‡ç»´åº¦ä¸Šå…ƒç´ çš„æœ€å¤§å€¼

```python
x1 = tf.constant([1., 2., 3.],dtype=tf.float64)
print(x1)

x2 = tf.cast(x1, tf.int32)
print(x2)
print (tf.reduce_min(x2), tf.reduce_max(x2))
```



- `tf.reduce_mean(å¼ é‡åï¼Œaxis=æ“ä½œè½´)`

  è®¡ç®—å¼ é‡æ²¿ç€æŒ‡å®šç»´åº¦çš„å¹³å‡å€¼

- `tf.reduce_sum(å¼ é‡åï¼Œaxis=æ“ä½œè½´)`

  è®¡ç®—å¼ é‡æ²¿ç€æŒ‡å®šç»´åº¦çš„å’Œ

```python
x=tf.constant([[1, 2, 3],
               [3, 2, 3]])
print(x)
print(tf.reduce_mean(x)) # æ‰€æœ‰æ•°å€¼çš„å‡å€¼
print(tf.reduce_mean(x, axis=0))
print(tf.reduce_sum(x, axis=1)) # æ‰€æœ‰æ•°å€¼çš„å’Œ >>>14
```



- axis

![image-20210909153000311](img/image-20210909153000311.png)



#### æ“ä½œå¼ é‡

- `tf.where()`
  æ¡ä»¶è¯­å¥çœŸè¿”å›Aï¼Œæ¡ä»¶è¯­å¥å‡è¿”å›B (å¼ é‡å†…éƒ¨å…ƒç´ å±‚é¢)
  `tf.where(æ¡ä»¶è¯­å¥ï¼Œ çœŸè¿”å›Aï¼Œ å‡è¿”å›B)`

  ```python
  a=tf.constant([1,2,3,1,1])
  b=tf.constant([0,1,3,4,5])
  c=tf.where(tf.greater(a, b), a, b) # è‹¥a>bï¼Œè¿”å›aå¯¹åº”ä½ç½®çš„å…ƒç´ ï¼Œå¦åˆ™è¿”å›bå¯¹åº”ä½ç½®çš„å…ƒç´ 
  print("c:", c)
  
  # cï¼štf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)
  ```



- `np.vstack()`

  å°†ä¸¤ä¸ªæ•°ç»„æŒ‰å‚ç›´æ–¹å‘å åŠ 
  `np.vstack(æ•°ç»„1ï¼Œæ•°ç»„2)`

  ```python
  import numpy as np
  a = np.array([1,2,3])
  b = np.array([4,5,6])
  c = np.vstack((a,b))
  print("c:\n",c)
  
  # c:
  # [[1 2 3]
  #  [4 5 6]]
  ```

  

- `np.mgrid[]`

  è¿”å›np.arrayæ•°ç»„ï¼Œå¯åŒæ—¶è¿”å›å¤šç»„ï¼Œæ¯ä¸ªæ•°ç»„å®šä¹‰ [èµ·å§‹å€¼ ç»“æŸå€¼ æ­¥é•¿)

  `np.mgrid[ èµ·å§‹å€¼: ç»“æŸå€¼: æ­¥é•¿ï¼Œèµ·å§‹å€¼: ç»“æŸå€¼: æ­¥é•¿, â€¦ ]`

- `x.ravel()`

  å¤šç»´æ•°ç»„å˜ä¸€ç»´æ•°ç»„ï¼Œå°†xå˜ä¸ºä¸€ç»´æ•°ç»„ï¼Œâ€œæŠŠ **.** å‰å˜é‡æ‹‰ç›´â€

- `np.c_[ æ•°ç»„1ï¼Œæ•°ç»„2ï¼Œâ€¦ ]`

  è¿”å›çš„æ•°ç»„å„å…ƒç´ é…å¯¹

```python
import numpyas np
x, y = np.mgrid[1:3:1, 2:4:0.5]
grid = np.c_[x.ravel(), y.ravel()]
print("x:",x)
print("y:",y)
print('grid:\n', grid)

è¿è¡Œç»“æœï¼š
x = [[1. 1. 1. 1.]
	 [2. 2. 2. 2.]]
y = [[2. 2.5 3. 3.5]
	 [2. 2.5 3. 3.5]]

grid:
    [[1. 2. ]
     [1. 2.5]
     [1. 3. ]
     [1. 3.5]
     [2. 2. ]
     [2. 2.5]
     [2. 3. ]
     [2. 3.5]]
```











### æ•°å­¦è¿ç®—

![image-20210909153802163](img/image-20210909153802163.png)

- å¯¹åº”å…ƒç´ å››åˆ™è¿ç®—

  >åªæœ‰ç»´åº¦ç›¸åŒçš„å¼ é‡æ‰å¯ä»¥åšå››åˆ™è¿ç®—

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸åŠ 
    `tf.add(å¼ é‡1ï¼Œå¼ é‡2)`

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸å‡
    `tf.subtract(å¼ é‡1ï¼Œå¼ é‡2)`

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸ä¹˜
    `tf.multiply(å¼ é‡1ï¼Œå¼ é‡2)`

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸é™¤
    `tf.divide(å¼ é‡1ï¼Œå¼ é‡2)`



```python
a = tf.ones([1, 3])
b = tf.fill([1, 3], 3.)
print(a)
print(b)
print(tf.add(a,b))
print(tf.subtract(a,b))
print(tf.multiply(a,b))
print(tf.divide(b,a))

# tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32
# tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)
```



- å¹³æ–¹ã€æ¬¡æ–¹ã€å¼€æ–¹
  - è®¡ç®—æŸä¸ªå¼ é‡çš„å¹³æ–¹
    `tf.square(å¼ é‡å)`
  - è®¡ç®—æŸä¸ªå¼ é‡çš„næ¬¡æ–¹
    `tf.pow(å¼ é‡åï¼Œnæ¬¡æ–¹æ•°)`
  - è®¡ç®—æŸä¸ªå¼ é‡çš„å¼€æ–¹
    `tf.sqrt(å¼ é‡åï¼‰`

```python
a = tf.fill([1, 2], 3.)
print(a)
print(tf.pow(a, 3))
print(tf.square(a))
print(tf.sqrt(a))

# tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)
# tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)
# tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)
# tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)
```



- çŸ©é˜µä¹˜
  - å®ç°ä¸¤ä¸ªçŸ©é˜µçš„ç›¸ä¹˜
    `tf.matmul(çŸ©é˜µ1ï¼ŒçŸ©é˜µ2)`

```python
a = tf.ones([3, 2])
b = tf.fill([2, 3], 3.)
print(tf.matmul(a, b))

# tf.Tensor([[6. 6. 6.]
#            [6. 6. 6.]
#            [6. 6. 6.]], shape=(3, 3), dtype=float32)
```



### æ–¹æ³•

#### `tf.Variable`

`tf.Variable()`å°†å˜é‡æ ‡è®°ä¸ºâ€œå¯è®­ç»ƒâ€ï¼Œè¢«æ ‡è®°çš„å˜é‡ä¼šåœ¨åå‘ä¼ æ’­ä¸­è®°å½•æ¢¯åº¦ä¿¡æ¯ã€‚ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ï¼Œå¸¸ç”¨è¯¥å‡½æ•°æ ‡è®°å¾…è®­ç»ƒå‚æ•°ã€‚

```python
tf.Variable(åˆå§‹å€¼)

w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))
```



#### `tf.data.Dataset.from_tensor_slices`

åˆ‡åˆ†ä¼ å…¥å¼ é‡çš„ç¬¬ä¸€ç»´åº¦ï¼Œç”Ÿæˆè¾“å…¥ç‰¹å¾/æ ‡ç­¾å¯¹ï¼Œæ„å»ºæ•°æ®é›†ï¼ˆNumpyå’ŒTensoræ ¼å¼éƒ½å¯ç”¨è¯¥è¯­å¥è¯»å…¥æ•°æ®ï¼‰

```python
data = tf.data.Dataset.from_tensor_slices((è¾“å…¥ç‰¹å¾, æ ‡ç­¾))

features = tf.random.normal([4, 3], mean=5, stddev=1)
labels = tf.constant([0, 1, 1, 0])
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
print(dataset)
for element in dataset:
	print(element)
    
# <TensorSliceDataset shapes: ((3,), ()), types: (tf.float32, tf.int32)>  ï¼ˆç‰¹å¾ï¼Œæ ‡ç­¾ï¼‰é…å¯¹
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5.0158296, 5.0271997, 6.399684 ],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=0>)
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5.150814 , 6.5250745, 5.037866 ],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=1>)
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([6.8031845, 5.809286 , 6.5262794],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=1>)
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4.4880443, 4.933515 , 5.1308255],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=0>)
```



#### `tf.GradientTape`

withç»“æ„è®°å½•è®¡ç®—è¿‡ç¨‹ï¼Œgradientæ±‚å‡ºå¼ é‡çš„æ¢¯åº¦

```python
with tf.GradientTape() as tape:
	è‹¥å¹²ä¸ªè®¡ç®—è¿‡ç¨‹
grad=tape.gradient(å‡½æ•°ï¼Œå¯¹è°æ±‚å¯¼)
```



```python
with tf.GradientTape() as tape:
	w = tf.Variable(tf.constant(3.0))
	loss = tf.pow(w, 2)
grad = tape.gradient(loss, w)
print(grad)

# tf.Tensor(6.0, shape=(), dtype=float32)
```



#### `enumerate`

enumerateæ˜¯pythonçš„å†…å»ºå‡½æ•°ï¼Œå®ƒå¯éå†æ¯ä¸ªå…ƒç´ (å¦‚åˆ—è¡¨ã€å…ƒç»„æˆ–å­—ç¬¦ä¸²)

ç»„åˆä¸ºï¼š<ç´¢å¼•ï¼Œå…ƒç´ >ï¼Œå¸¸åœ¨forå¾ªç¯ä¸­ä½¿ç”¨ã€‚
`enumerate(åˆ—è¡¨å)`

```python
seq= ['one', 'two', 'three']
for i, element in enumerate(seq):
	print(i, element)

# 0 one
# 1 two
# 2 three
```



#### `tf.one_hot`

ç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodingï¼‰ï¼šåœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¸¸ç”¨ç‹¬çƒ­ç åšæ ‡ç­¾ï¼Œæ ‡è®°ç±»åˆ«ï¼š1è¡¨ç¤ºæ˜¯ï¼Œ0è¡¨ç¤ºé

`tf.one_hot(å¾…è½¬æ¢æ•°æ®, depth=å‡ åˆ†ç±»)`

```python
classes = 3
labels = tf.constant([1,0,2]) # è¾“å…¥çš„å…ƒç´ å€¼æœ€å°ä¸º0ï¼Œæœ€å¤§ä¸º2
output = tf.one_hot(labels, depth=classes )
print(output)

# [[0. 1. 0.]
#  [1. 0. 0.]
#  [0. 0. 1.]], shape=(3, 3), dtype=float32)
```



#### `tf.nn.softmax`

![image-20210909161915094](img/image-20210909161915094.png)



```python
y = tf.constant( [1.01, 2.01, -0.66] )
y_pro= tf.nn.softmax(y)
print("After softmax, y_prois:", y_pro)

# After softmax, y_prois: tf.Tensor([0.255981740.695830460.0481878], shape=(3,), dtype=float32)
```



#### `assign_sub`

èµ‹å€¼æ“ä½œï¼Œæ›´æ–°å‚æ•°çš„å€¼å¹¶è¿”å›

è°ƒç”¨assign_subå‰ï¼Œå…ˆç”¨tf.Variableå®šä¹‰å˜é‡wä¸ºå¯è®­ç»ƒï¼ˆå¯è‡ªæ›´æ–°ï¼‰

`w.assign_sub(wè¦è‡ªå‡çš„å†…å®¹)`

```python
w = tf.Variable(4)
w.assign_sub(1)
print(w)  # w-=1
```



#### `tf.argmax`

è¿”å›å¼ é‡æ²¿æŒ‡å®šç»´åº¦æœ€å¤§å€¼çš„ç´¢å¼•
`tf.argmax(å¼ é‡å,axis=æ“ä½œè½´)`

```python
import numpyas np
test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])
print(test)
print(tf.argmax(test, axis=0)) # è¿”å›æ¯ä¸€åˆ—æœ€å¤§å€¼çš„ç´¢å¼•
print(tf.argmax(test, axis=1)) # è¿”å›æ¯ä¸€è¡Œæœ€å¤§å€¼çš„ç´¢å¼•

# [[1 2 3]
#  [2 3 4]
#  [5 4 3]
#  [8 7 2]]
# tf.Tensor([3 3 1], shape=(3,), dtype=int64)
# tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)
```



### æ•°æ®é›†è¯»å–

- é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆIrisï¼‰

  ```python
  from sklearn.datasetsimport load_iris
  x_data= datasets.load_iris().data   # è¿”å›irisæ•°æ®é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾
  y_data= datasets.load_iris().target # è¿”å›irisæ•°æ®é›†æ‰€æœ‰æ ‡ç­¾
  ```



## å­¦ä¹ ç‡

![image-20210910151223855](img/image-20210910151223855.png)



- æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
  å¯ä»¥å…ˆç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼Œå¿«é€Ÿå¾—åˆ°è¾ƒä¼˜è§£ï¼Œç„¶åé€æ­¥å‡å°å­¦ä¹ ç‡ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒåæœŸç¨³å®šã€‚

  `æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡= åˆå§‹å­¦ä¹ ç‡* å­¦ä¹ ç‡è¡°å‡ç‡ï¼ˆå½“å‰è½®æ•°/ å¤šå°‘è½®è¡°å‡ä¸€æ¬¡ï¼‰`



## æ¿€æ´»å‡½æ•°

![image-20210910151737249](img/image-20210910151737249.png)

![image-20210910220550382](img/image-20210910220550382.png)



- sigmoid

  ![image-20210910151840754](img/image-20210910151840754.png)

  ç‰¹ç‚¹
  ï¼ˆ1ï¼‰æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±

  â€‹		æ¢¯åº¦å¤§å° (0, 0.25] ï¼Œç´¯ä¹˜åé€ æˆæ¢¯åº¦æ¶ˆå¤±

  ï¼ˆ2ï¼‰è¾“å‡ºé0æ­£æ•°ï¼Œå‡å€¼é0ï¼Œæ”¶æ•›æ…¢

  ï¼ˆ3ï¼‰å¹‚è¿ç®—å¤æ‚ï¼Œè®­ç»ƒæ—¶é—´é•¿



- Tanh

  ![image-20210910151933750](img/image-20210910151933750.png)

  ç‰¹ç‚¹
  ï¼ˆ1ï¼‰è¾“å‡ºæ˜¯0å‡å€¼

  ï¼ˆ2ï¼‰æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±

  ï¼ˆ3ï¼‰å¹‚è¿ç®—å¤æ‚ï¼Œè®­ç»ƒæ—¶é—´é•¿



- Relu

  ![image-20210910152241269](img/image-20210910152241269.png)

  

  ä¼˜ç‚¹ï¼š

  ï¼ˆ1ï¼‰è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜(åœ¨æ­£åŒºé—´)

  ï¼ˆ2ï¼‰åªéœ€åˆ¤æ–­è¾“å…¥æ˜¯å¦å¤§äº0ï¼Œè®¡ç®—é€Ÿåº¦å¿«

  ï¼ˆ3ï¼‰æ”¶æ•›é€Ÿåº¦è¿œå¿«äºsigmoidå’Œtanh
  ç¼ºç‚¹ï¼š
  ï¼ˆ1ï¼‰è¾“å‡ºé0å‡å€¼ï¼Œæ”¶æ•›æ…¢

  ï¼ˆ2ï¼‰Dead RelUé—®é¢˜ï¼šæŸäº›ç¥ç»å…ƒå¯èƒ½æ°¸è¿œä¸ä¼šè¢«æ¿€æ´»ï¼Œå¯¼è‡´ç›¸åº”çš„å‚æ•°æ°¸è¿œä¸èƒ½è¢«æ›´æ–°ã€‚



- Leaky Relu

  ![image-20210910152543206](img/image-20210910152543206.png)



## æŸå¤±å‡½æ•°

![image-20210910221002716](img/image-20210910221002716.png)



### å‡æ–¹è¯¯å·®

![image-20210910221256343](img/image-20210910221256343.png)



### äº¤å‰ç†µ

![image-20210910221422551](img/image-20210910221422551.png)

```python
loss_ce1=tf.losses.categorical_crossentropy([1,0],[0.6,0.4])
loss_ce2=tf.losses.categorical_crossentropy([1,0],[0.8,0.2])
print("loss_ce1:", loss_ce1)
print("loss_ce2:", loss_ce2)

è¿è¡Œç»“æœï¼š
loss_ce1: tf.Tensor(0.5108256, shape=(), dtype=float32)
loss_ce2: tf.Tensor(0.2231435, shape=(), dtype=float32)
    
```



- `softmax`ä¸äº¤å‰ç†µç»“åˆ

  è¾“å‡ºå…ˆè¿‡`softmax`å‡½æ•°ï¼Œå†è®¡ç®—yä¸y_çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

  `tf.nn.softmax_cross_entropy_with_logits(y_ï¼Œy)`

  ```python
  y_ = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])
  y = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]])
  y_pro = tf.nn.softmax(y)
  loss_ce1 = tf.losses.categorical_crossentropy(y_, y_pro)
  loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)
  print('åˆ†æ­¥è®¡ç®—çš„ç»“æœ:\n', loss_ce1)
  print('ç»“åˆè®¡ç®—çš„ç»“æœ:\n', loss_ce2)
  
  åˆ†æ­¥è®¡ç®—çš„ç»“æœ:
  tf.Tensor([1.68795487e-041.03475622e-036.58839038e-022.58349207e+005.49852354e-02],shape=(5,),dtype=float64)
  ç»“åˆè®¡ç®—çš„ç»“æœ:
  tf.Tensor([1.68795487e-041.03475622e-036.58839038e-022.58349207e+005.49852354e-02],shape=(5,),dtype=float64)
  ```

  

### æ‰‹å†™æ¢¯åº¦ä¸‹é™è¿‡ç¨‹

```python
w = tf.Variable(tf.constant(5, dtype=tf.float32))  # åˆå§‹åŒ–æ—¶å€™èµ‹å€¼ä¸º5
lr = 0.01
epoch = 400

for epoch in range(epoch):  # for epoch å®šä¹‰é¡¶å±‚å¾ªç¯ï¼Œè¡¨ç¤ºå¯¹æ•°æ®é›†å¾ªç¯ epoch æ¬¡
    with tf.GradientTape() as tape:  # with ç»“æ„åˆ° grads æ¡†èµ·äº†æ¢¯åº¦çš„è®¡ç®—è¿‡ç¨‹
        loss = tf.square(w + 1)      # loss é€šè¿‡è°ƒæ•´ w çš„å€¼ä½¿ loss æœ€å°
    grads = tape.gradient(loss, w)   # .gradient å‡½æ•°å‘ŠçŸ¥è°å¯¹è°æ±‚å¯¼

    w.assign_sub(lr * grads)  # .assign_sub å¯¹å˜é‡åšè‡ªå‡ w -= lr*grads
    print("After %s epoch,w is %f,loss is %f" % (epoch, w.numpy(), loss))
```





## æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ

![image-20210910222051194](img/image-20210910222051194.png)



### æ­£åˆ™é¡¹

![image-20210910222200719](img/image-20210910222200719.png)



- L1å’ŒL2

  ![image-20210910222236869](img/image-20210910222236869.png)

  

```python
with tf.GradientTape() as tape:  # è®°å½•æ¢¯åº¦ä¿¡æ¯
    h1 = tf.matmul(x_train, w1) + b1  # è®°å½•ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
    h1 = tf.nn.relu(h1)
    y = tf.matmul(h1, w2) + b2

    # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
    loss_mse = tf.reduce_mean(tf.square(y_train - y))

    # æ·»åŠ l2æ­£åˆ™åŒ–
    loss_regularization = []
    # å†…éƒ¨ç»†èŠ‚ tf.nn.l2_loss(w) = sum(w ** 2) / 2
    loss_regularization.append(tf.nn.l2_loss(w1))
    loss_regularization.append(tf.nn.l2_loss(w2))
    # æ±‚å’Œï¼Œæ±‚è§£æ­£åˆ™é¡¹
    loss_regularization = tf.reduce_sum(loss_regularization)

    loss = loss_mse + 0.03 * loss_regularization  # REGULARIZER = 0.03

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
variables = [w1, b1, w2, b2]
grads = tape.gradient(loss, variables)

# å®ç°æ¢¯åº¦æ›´æ–°
# w1 = w1 - lr * w1_grad
w1.assign_sub(lr * grads[0])
b1.assign_sub(lr * grads[1])
w2.assign_sub(lr * grads[2])
b2.assign_sub(lr * grads[3])
```



## ä¼˜åŒ–å™¨

![image-20210910223516582](img/image-20210910223516582.png)



### SGD

![image-20210910223604954](img/image-20210910223604954.png)



### SGDM

![image-20210910223721645](img/image-20210910223721645.png)



```python
m_w, m_b = 0, 0
beta = 0.9

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# sgd-momentun  
m_w = beta * m_w + (1 - beta) * grads[0]
m_b = beta * m_b + (1 - beta) * grads[1]
w1.assign_sub(lr * m_w)
b1.assign_sub(lr * m_b)
```



### Adagrad

![image-20210910224027009](img/image-20210910224027009.png)



```python
# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# adagrad
v_w += tf.square(grads[0])
v_b += tf.square(grads[1])
w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))
```



### RMSProp

![image-20210910224242736](img/image-20210910224242736.png)



```python
v_w, v_b = 0, 0
beta = 0.9

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# rmsprop
v_w = beta * v_w + (1 - beta) * tf.square(grads[0])
v_b = beta * v_b + (1 - beta) * tf.square(grads[1])
w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))
```



### Adam

![image-20210910224559530](img/image-20210910224559530.png)



```python
m_w, m_b = 0, 0
v_w, v_b = 0, 0
beta1, beta2 = 0.9, 0.999
delta_w, delta_b = 0, 0
global_step = 0

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# adam
m_w = beta1 * m_w + (1 - beta1) * grads[0]
m_b = beta1 * m_b + (1 - beta1) * grads[1]
v_w = beta2 * v_w + (1 - beta2) * tf.square(grads[0])
v_b = beta2 * v_b + (1 - beta2) * tf.square(grads[1])

m_w_correction = m_w / (1 - tf.pow(beta1, int(global_step)))
m_b_correction = m_b / (1 - tf.pow(beta1, int(global_step)))
v_w_correction = v_w / (1 - tf.pow(beta2, int(global_step)))
v_b_correction = v_b / (1 - tf.pow(beta2, int(global_step)))

w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))
b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))
```



## æ­å»ºç½‘ç»œå…«è‚¡

**å…­æ­¥æ³•ï¼š**

1. **import**
2. **train, test**
3. **model = tf.keras.models.Sequential**
4. **model.compile**
5. **model.fit**
6. **model.summary**



### Basesline

- IRIS-Sequential

  ```python
  import tensorflow as tf
  from sklearn import datasets
  import numpy as np
  
  x_train = datasets.load_iris().data
  y_train = datasets.load_iris().target
  
  np.random.seed(116)
  np.random.shuffle(x_train)
  np.random.seed(116)
  np.random.shuffle(y_train)
  tf.random.set_seed(116)
  
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())
  ])
  
  model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy'])
  
  model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)
  
  model.summary()
  ```

  

- MNIST-Sequential

  ![image-20210913185128467](img/image-20210913185128467.png)

  ```python
  import tensorflow as tf
  
  mnist = tf.keras.datasets.mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(),  # æ‹‰ç›´
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy']  # y_ï¼ˆçœŸå®ï¼‰æ˜¯æ•°å€¼ï¼Œyï¼ˆé¢„æµ‹ï¼‰æ˜¯ç‹¬çƒ­ç (æ¦‚ç‡åˆ†å¸ƒ)
               )
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  
  model.summary()
  ```




- FASHION-Sequential

  ```python
  import tensorflow as tf
  
  fashion = tf.keras.datasets.fashion_mnist
  (x_train, y_train),(x_test, y_test) = fashion.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy'])
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  model.summary()
  ```

  





### ç¬¬ä¸€æ­¥ï¼š`import`



### ç¬¬äºŒæ­¥ï¼š`trainã€test`

- è‡ªåˆ¶æ•°æ®é›†

  - å›¾ç‰‡è·¯å¾„ä¸‹å­˜å‚¨å›¾ç‰‡

  ![image-20210913214015589](img/image-20210913214015589.png)

  - æ ‡ç­¾æ–‡ä»¶ï¼ˆå›¾ç‰‡åç§°ï¼Œæ ‡ç­¾ï¼‰

    ![image-20210913214141104](img/image-20210913214141104.png)

  

  ```python
  # def generateds(å›¾ç‰‡è·¯å¾„, æ ‡ç­¾æ–‡ä»¶): 
  # åŠ è½½å›¾ç‰‡ è½¬æ¢æˆçŸ©é˜µ
  
  def generateds(path, txt):
      f = open(txt, 'r')
      contents = f.readlines()  # æŒ‰è¡Œè¯»å–
      f.close()
      x, y_ = [], []
      for content in contents:
          value = content.split()  # ä»¥ç©ºæ ¼åˆ†å¼€ï¼Œå­˜å…¥æ•°ç»„
          img_path = path + value[0]
          img = Image.open(img_path)
          img = np.array(img.convert('L'))
          img = img / 255.
          x.append(img)
          y_.append(value[1])
      print('loading : ' + "data")
  
      x = np.array(x)
      y_ = np.array(y_)
      y_ = y_.astype(np.int64)
      return x, y_
  ```

  

- æ•°æ®å¢å¼º

  ```python
  image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator(
                      rescale=æ‰€æœ‰æ•°æ®å°†ä¹˜ä»¥è¯¥æ•°å€¼
                      rotation_range=éšæœºæ—‹è½¬è§’åº¦æ•°èŒƒå›´
                      width_shift_range=éšæœºå®½åº¦åç§»é‡
                      height_shift_range=éšæœºé«˜åº¦åç§»é‡
                      æ°´å¹³ç¿»è½¬ï¼šhorizontal_flip=æ˜¯å¦éšæœºæ°´å¹³ç¿»è½¬
                      éšæœºç¼©æ”¾ï¼šzoom_range= éšæœºç¼©æ”¾çš„èŒƒå›´[1-nï¼Œ1+n])
  
  # ä¾‹ï¼šæ•°æ®å¢å¼ºï¼ˆå¢å¤§æ•°æ®é‡ï¼‰
  image_gen_train = ImageDataGenerator(
                      rescale=1. / 1.,   # å¦‚ä¸ºå›¾åƒï¼Œåˆ†æ¯ä¸º255æ—¶ï¼Œå¯å½’è‡³0ï½1
                      rotation_range=45, # éšæœº45åº¦æ—‹è½¬
      				width_shift_range=.15,  # å®½åº¦åç§»
                      height_shift_range=.15, # é«˜åº¦åç§»
      				horizontal_flip=False,  # æ°´å¹³ç¿»è½¬
                      zoom_range=0.5 	   # å°†å›¾åƒéšæœºç¼©æ”¾é˜ˆé‡50ï¼…
  					)
  
  
  # ç¬¬ä¸€æ­¥
  # (60000, 28, 28) --> (60000, 28, 28, 1)
  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
  
  # ç¬¬äºŒæ­¥
  image_gen_train.fit(x_train)
  
  # ç¬¬ä¸‰æ­¥
  # Baseline: model.fit(x_train, y_train, batch_size=32, â€¦â€¦)
  model.fit(image_gen_train.flow(x_train, y_train,batch_size=32), â€¦â€¦)
  ```

  

### ç¬¬ä¸‰æ­¥ï¼š`Sequential`

`model = tf.keras.models.Sequential([ç½‘ç»œç»“æ„])`  # æè¿°å„å±‚ç½‘ç»œ

- ç½‘ç»œç»“æ„ä¸¾ä¾‹ï¼š
  - æ‹‰ç›´å±‚ï¼š`tf.keras.layers.Flatten()`

  - å…¨è¿æ¥å±‚ï¼š`tf.keras.layers.Dense(ç¥ç»å…ƒä¸ªæ•°, activation="æ¿€æ´»å‡½æ•°", kernel_regularizer=å“ªç§æ­£åˆ™åŒ–)`

    - `activation`ï¼ˆå­—ç¬¦ä¸²ç»™å‡ºï¼‰å¯é€‰: reluã€softmaxã€sigmoid ã€tanh

    - `kernel_regularizer`å¯é€‰:`tf.keras.regularizers.l1()`ã€`tf.keras.regularizers.l2()`



### ç¬¬å››æ­¥ï¼š`model.compile`

`model.compile(optimizer = æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨, loss = æŸå¤±å‡½æ•°, metrics = [â€œå‡†ç¡®ç‡â€] )`

- `optimizer`å¯é€‰:
  
  - â€˜sgdâ€™ or `tf.keras.optimizers.SGD(lr=å­¦ä¹ ç‡,momentum=åŠ¨é‡å‚æ•°)`
  - â€˜adagradâ€™ or `tf.keras.optimizers.Adagrad(lr=å­¦ä¹ ç‡)`
  - â€˜adadeltaâ€™ or `tf.keras.optimizers.Adadelta(lr=å­¦ä¹ ç‡)`
  - â€˜adamâ€™or `tf.keras.optimizers.Adam(lr=å­¦ä¹ ç‡, beta_1=0.9, beta_2=0.999)`
  
  
- `loss`å¯é€‰:
  - `mse` or `tf.keras.losses.MeanSquaredError()` 
    - å›å½’é—®é¢˜---å‡æ–¹è¯¯å·®
  - `sparse_categorical_crossentropy`or `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)`
    - å¤šåˆ†ç±»---äº¤å‰ç†µ
    - `from_logits=False` ç½‘ç»œæœ€åè¿›è¡Œäº† softmax æ¦‚ç‡å½’ä¸€åŒ–
    - `from_logits=True`



- `metrics`å¯é€‰:
  - `accuracy`ï¼šy\_å’Œyéƒ½æ˜¯æ•°å€¼ï¼Œå¦‚ y\_=[1]ï¼Œy=[1]
  - `categorical_accuracy`ï¼šy\_å’Œyéƒ½æ˜¯ç‹¬çƒ­ç (æ¦‚ç‡åˆ†å¸ƒ)ï¼Œå¦‚ y_=[0,1,0]ï¼Œy=[0.256,0.695,0.048]
  - `sparse_categorical_accuracy`ï¼šy\_æ˜¯æ•°å€¼ï¼Œyæ˜¯ç‹¬çƒ­ç (æ¦‚ç‡åˆ†å¸ƒ)ï¼Œå¦‚ y\_=[1]ï¼Œy=[0.256,0.695,0.048]



### ç¬¬äº”æ­¥ï¼š`model.fit`

```python
model.fit(è®­ç»ƒé›†çš„è¾“å…¥ç‰¹å¾, è®­ç»ƒé›†çš„æ ‡ç­¾, 
          batch_size= , epochs= ,
          validation_data=(æµ‹è¯•é›†çš„è¾“å…¥ç‰¹å¾ï¼Œæµ‹è¯•é›†çš„æ ‡ç­¾),
          validation_split=ä»è®­ç»ƒé›†åˆ’åˆ†å¤šå°‘æ¯”ä¾‹ç»™æµ‹è¯•é›†,
          validation_freq= å¤šå°‘æ¬¡epochæµ‹è¯•ä¸€æ¬¡)
```



### ç¬¬å…­æ­¥ï¼š`model.summary`

![image-20210913183347002](img/image-20210913183347002.png)







## ç»§æ‰¿`Model`

- åŸºç¡€ç»“æ„

```python
classMyModel(Model):
	def __init__(self):
		super(MyModel, self).__init__()
		å®šä¹‰ç½‘ç»œç»“æ„å—
	def call(self, x):
		è°ƒç”¨ç½‘ç»œç»“æ„å—ï¼Œå®ç°å‰å‘ä¼ æ’­
		return y
    
model = MyModel() # æ¨¡å‹å®ä¾‹åŒ–

# ==============================================
class IrisModel(Model):
    def __init__(self):
        super(IrisModel, self).__init__()
        self.d1 = Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())

    def call(self, x):
        y = self.d1(x)
        return y

model = IrisModel()
```



- MNIST-Model

  ```python
  import tensorflow as tf
  from tensorflow.keras.layers import Dense, Flatten
  from tensorflow.keras import Model
  
  mnist = tf.keras.datasets.mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  
  class MnistModel(Model):
      def __init__(self):
          super(MnistModel, self).__init__()
          self.flatten = Flatten()
          self.d1 = Dense(128, activation='relu')
          self.d2 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.flatten(x)
          x = self.d1(x)
          y = self.d2(x)
          return y
  
  
  model = MnistModel()
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy']
               )
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  
  model.summary()
  ```

  

- FASHION-Model

  ```python
  import tensorflow as tf
  from tensorflow.keras.layers import Dense, Flatten
  from tensorflow.keras import Model
  
  fashion = tf.keras.datasets.fashion_mnist
  (x_train, y_train),(x_test, y_test) = fashion.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  
  class MnistModel(Model):
      def __init__(self):
          super(MnistModel, self).__init__()
          self.flatten = Flatten()
          self.d1 = Dense(128, activation='relu')
          self.d2 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.flatten(x)
          x = self.d1(x)
          y = self.d2(x)
          return y
  
  
  model = MnistModel()
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy'])
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  model.summary()
  
  ```



## è¯»å–ä¿å­˜æ¨¡å‹

### è¯»å–æ¨¡å‹

```python
# è¯»å–æ¨¡å‹
checkpoint_save_path = "./checkpoint/model_name.ckpt"
if os.path.exists(checkpoint_save_path + '.index'):
    print('-------------load the model-----------------')
    # checkpoint_save_path åœ°å€ä¸åŠ  '.index'
    model.load_weights(checkpoint_save_path) 
```



### ä¿å­˜æ¨¡å‹

```python
my_callback = tf.keras.callbacks.ModelCheckpoint(
                    filepath=è·¯å¾„æ–‡ä»¶å, 
                    save_weights_only=True/False, 
                    save_best_only=True/False)

history = model.fit(callbacks=[my_callback])


# ä¾‹ =====================================
my_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                 save_weights_only=True,
                                                 save_best_only=True)
history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[my_callback])
```



### å‚æ•°æå–

- è¿”å›æ¨¡å‹ä¸­å¯è®­ç»ƒçš„å‚æ•°:

  `model.trainable_variables`

- è®¾ç½®printè¾“å‡ºæ ¼å¼

  `np.set_printoptions(threshold=è¶…è¿‡å¤šå°‘çœç•¥æ˜¾ç¤º)`

```python
np.set_printoptions(threshold=np.inf) # np.infè¡¨ç¤ºæ— é™å¤§
print(model.trainable_variables)

file = open('./weights.txt', 'w')
for v in model.trainable_variables:
    file.write(str(v.name) + '\n')
    file.write(str(v.shape) + '\n')
    file.write(str(v.numpy()) + '\n')
file.close()
```



## ç»˜åˆ¶`acc/loss`æ›²çº¿

```python
history=model.fit(è®­ç»ƒé›†æ•°æ®, 
                  è®­ç»ƒé›†æ ‡ç­¾, 
                  batch_size=, 
                  epochs=,
                  validation_split=ç”¨ä½œæµ‹è¯•æ•°æ®çš„æ¯”ä¾‹,
                  validation_data=æµ‹è¯•é›†,
                  validation_freq=æµ‹è¯•é¢‘ç‡)
```



- historyï¼š

  - è®­ç»ƒé›†lossï¼š`loss`
  - æµ‹è¯•é›†lossï¼š`val_loss`
  - è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š`sparse_categorical_accuracy`
  - æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š`val_sparse_categorical_accuracy`

  ```python
  acc = history.history['sparse_categorical_accuracy']
  val_acc = history.history['val_sparse_categorical_accuracy']
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  ```



- ç»˜å›¾

```python
from matplotlib import pyplot as plt

# æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„accå’Œlossæ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.show()
```



## æ¨¡å‹é¢„æµ‹

- è¿”å›å‰å‘ä¼ æ’­è®¡ç®—ç»“æœ

  `predict(è¾“å…¥ç‰¹å¾, batch_size=æ•´æ•°)`

```python
# æ•°æ®é¢„å¤„ç†
result = model.predict(x_predict)
pred = tf.argmax(result, axis=1) # å–æ¦‚ç‡æœ€å¤§å€¼

acc = tf.keras.metrics.Accuracy()
acc.update_state(y_test, pred) # y_test=çœŸå®æ ‡ç­¾, pred=é¢„æµ‹å€¼
print(acc.result().numpy())    # 0.9 å°æ•°å€¼
```



# æ‰‹å†™ç¥ç»ç½‘ç»œIrisåˆ†ç±»

```python
# å¯¼å…¥æ‰€éœ€æ¨¡å—
import tensorflow as tf
from sklearn import datasets
from matplotlib import pyplot as plt
import numpy as np

# ====================================================================

# å¯¼å…¥æ•°æ®ï¼Œåˆ†åˆ«ä¸ºè¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target
print(x_data.shape, y_data.shape)

# éšæœºæ‰“ä¹±æ•°æ®ï¼ˆå› ä¸ºåŸå§‹æ•°æ®æ˜¯é¡ºåºçš„ï¼Œé¡ºåºä¸æ‰“ä¹±ä¼šå½±å“å‡†ç¡®ç‡ï¼‰
# seed: éšæœºæ•°ç§å­ï¼Œæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå½“è®¾ç½®ä¹‹åï¼Œæ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°éƒ½ä¸€æ ·
np.random.seed(116)  # ä½¿ç”¨ç›¸åŒçš„seedï¼Œä¿è¯è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾ä¸€ä¸€å¯¹åº”
np.random.shuffle(x_data)
np.random.seed(116)
np.random.shuffle(y_data)
tf.random.set_seed(116)

# å°†æ‰“ä¹±åçš„æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†ä¸ºå‰120è¡Œï¼Œæµ‹è¯•é›†ä¸ºå30è¡Œ
x_train = x_data[:-30]
y_train = y_data[:-30]
x_test = x_data[-30:]
y_test = y_data[-30:]

# è½¬æ¢xçš„æ•°æ®ç±»å‹ï¼Œå¦åˆ™åé¢çŸ©é˜µç›¸ä¹˜æ—¶ä¼šå› æ•°æ®ç±»å‹ä¸ä¸€è‡´æŠ¥é”™
x_train = tf.cast(x_train, tf.float32)
x_test = tf.cast(x_test, tf.float32)

# from_tensor_sliceså‡½æ•°ä½¿è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾å€¼ä¸€ä¸€å¯¹åº”ã€‚ï¼ˆæŠŠæ•°æ®é›†åˆ†æ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡batchç»„æ•°æ®ï¼‰
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

# ====================================================================

# ç”Ÿæˆç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œ4ä¸ªè¾“å…¥ç‰¹å¾æ•…ï¼Œè¾“å…¥å±‚ä¸º4ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼›å› ä¸º3åˆ†ç±»ï¼Œæ•…è¾“å‡ºå±‚ä¸º3ä¸ªç¥ç»å…ƒ
# ç”¨tf.Variable()æ ‡è®°å‚æ•°å¯è®­ç»ƒ
# ä½¿ç”¨seedä½¿æ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°ç›¸åŒ
w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))
b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))

lr = 0.1  # å­¦ä¹ ç‡ä¸º0.1
train_loss_results = []  # å°†æ¯æ¬¡è¿­ä»£çš„lossè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»lossæ›²çº¿æä¾›æ•°æ®
test_acc = []  # å°†æ¯æ¬¡è¿­ä»£çš„accè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»accæ›²çº¿æä¾›æ•°æ®
epoch = 500  # è¿­ä»£500æ¬¡
loss_all = 0  # æ¯æ¬¡è¿­ä»£åˆ†å¤šä¸ªbatchï¼Œloss_allè®°å½•æ‰€æœ‰batchç”Ÿæˆçš„losså’Œ

# ====================================================================

# è®­ç»ƒéƒ¨åˆ†
for epoch in range(epoch):  # æ•°æ®é›†çº§åˆ«çš„å¾ªç¯ï¼Œæ¯ä¸ªepochå¾ªç¯ä¸€æ¬¡å®Œæ•´æ•°æ®é›†
    for step, (x_train, y_train) in enumerate(train_db):  # batchçº§åˆ«çš„å¾ªç¯ ï¼Œæ¯ä¸ªstepå¾ªç¯ä¸€ä¸ªbatch
        with tf.GradientTape() as tape:  # withç»“æ„è®°å½•æ¢¯åº¦ä¿¡æ¯
            y = tf.matmul(x_train, w1) + b1  # ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
            y = tf.nn.softmax(y)  # ä½¿è¾“å‡ºyç¬¦åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ­¤æ“ä½œåä¸ç‹¬çƒ­ç åŒé‡çº§ï¼Œå¯ç›¸å‡æ±‚lossï¼‰
            y_ = tf.one_hot(y_train, depth=3)  # å°†æ ‡ç­¾å€¼è½¬æ¢ä¸ºç‹¬çƒ­ç æ ¼å¼ï¼Œæ–¹ä¾¿è®¡ç®—losså’Œaccuracy
            loss = tf.reduce_mean(tf.square(y_ - y))  # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
            loss_all += loss.numpy()  # å°†æ¯ä¸ªstepè®¡ç®—å‡ºçš„lossç´¯åŠ ï¼Œä¸ºåç»­æ±‚losså¹³å‡å€¼æä¾›æ•°æ®ï¼Œè¿™æ ·è®¡ç®—çš„lossæ›´å‡†ç¡®
        # è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
        grads = tape.gradient(loss, [w1, b1])

        # å®ç°æ¢¯åº¦æ›´æ–°
        w1.assign_sub(lr * grads[0])  # å‚æ•°w1è‡ªæ›´æ–° w1 = w1 - lr * w1_grad
        b1.assign_sub(lr * grads[1])  # å‚æ•°bè‡ªæ›´æ–°  b = b - lr * b_grad

    # æ¯ä¸ªepochï¼Œæ‰“å°lossä¿¡æ¯
    if epoch % 50 == 0:
        print("Epoch {}, loss: {}".format(epoch, loss_all / 4))
    train_loss_results.append(loss_all / 4)  # å°†4ä¸ªstepçš„lossæ±‚å¹³å‡è®°å½•åœ¨æ­¤å˜é‡ä¸­
    loss_all = 0  # loss_allå½’é›¶ï¼Œä¸ºè®°å½•ä¸‹ä¸€ä¸ªepochçš„lossåšå‡†å¤‡

    # æµ‹è¯•éƒ¨åˆ†
    # total_correctä¸ºé¢„æµ‹å¯¹çš„æ ·æœ¬ä¸ªæ•°, total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œå°†è¿™ä¸¤ä¸ªå˜é‡éƒ½åˆå§‹åŒ–ä¸º0
    total_correct, total_number = 0, 0
    for x_test, y_test in test_db:
        # ä½¿ç”¨æ›´æ–°åçš„å‚æ•°è¿›è¡Œé¢„æµ‹
        y = tf.matmul(x_test, w1) + b1
        y = tf.nn.softmax(y)
        # è¿”å›yä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„åˆ†ç±»
        pred = tf.argmax(y, axis=1)
        # å°†predè½¬æ¢ä¸ºy_testçš„æ•°æ®ç±»å‹
        pred = tf.cast(pred, dtype=y_test.dtype)
        
        # è‹¥åˆ†ç±»æ­£ç¡®ï¼Œåˆ™correct=1ï¼Œå¦åˆ™ä¸º0ï¼Œå°†boolå‹çš„ç»“æœè½¬æ¢ä¸ºintå‹
        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)
        # å°†æ¯ä¸ªbatchçš„correctæ•°åŠ èµ·æ¥
        correct = tf.reduce_sum(correct)
        
        # å°†æ‰€æœ‰batchä¸­çš„correctæ•°åŠ èµ·æ¥
        total_correct += int(correct)
        # total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œä¹Ÿå°±æ˜¯x_testçš„è¡Œæ•°ï¼Œshape[0]è¿”å›å˜é‡çš„è¡Œæ•°
        total_number += x_test.shape[0]
        
    # æ€»çš„å‡†ç¡®ç‡ç­‰äºtotal_correct/total_number
    acc = total_correct / total_number
    test_acc.append(acc)
    if epoch % 50 == 0:
        print("Test_acc:", acc)
        print("--------------------------")
        
# ====================================================================  

# ç»˜åˆ¶ loss æ›²çº¿
plt.title('Loss Function Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')  # xè½´å˜é‡åç§°
plt.ylabel('Loss')   # yè½´å˜é‡åç§°
plt.plot(train_loss_results, label="$Loss$")  # é€ç‚¹ç”»å‡ºtrian_loss_resultså€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Loss
plt.legend()  # ç”»å‡ºæ›²çº¿å›¾æ ‡
plt.show()    # ç”»å‡ºå›¾åƒ

# ç»˜åˆ¶ Accuracy æ›²çº¿
plt.title('Acc Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')     # xè½´å˜é‡åç§°
plt.ylabel('Acc')       # yè½´å˜é‡åç§°
plt.plot(test_acc, label="$Accuracy$")  # é€ç‚¹ç”»å‡ºtest_accå€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Accuracy
plt.legend()
plt.show()
```



# å·ç§¯ç¥ç»ç½‘ç»œ

> å·ç§¯ç¥ç»ç½‘ç»œï¼šå€ŸåŠ©å·ç§¯æ ¸æå–ç‰¹å¾åï¼Œé€å…¥å…¨è¿æ¥ç½‘ç»œ

![image-20210914142029510](img/image-20210914142029510.png)

![image-20210914142135724](img/image-20210914142135724.png)

CBAPD

- C: å·ç§¯
  - ä¸€å¼ åŸå›¾ï¼ˆ32ï¼Œ32ï¼Œ3ï¼‰**ï¼ï¼ï¼**ç»è¿‡kä¸ªå·ç§¯æ ¸ç”Ÿæˆï½‹å¼ äºŒç»´ç‰¹å¾å›¾ï¼ˆ32ï¼Œ32ï¼‰
- B: æ‰¹æ ‡å‡†åŒ–
  - æ•°æ®æ ¼å¼ä¸ä¼šå‘ç”Ÿæ”¹å˜
- A: æ¿€æ´»å‡½æ•°
  - æ‰€æœ‰å…ƒç´ ç»è¿‡æ¿€æ´»å‡½æ•°æ˜ å°„ï¼ˆæ— å¯è®­ç»ƒå‚æ•°ï¼‰
- P: æ± åŒ–
- D: èˆå¼ƒ
  - éšæœºä¸¢å¼ƒå…ƒç´ 



## å·ç§¯è®¡ç®—

> å·ç§¯è®¡ç®—å¯è®¤ä¸ºæ˜¯ä¸€ç§æœ‰æ•ˆæå–å›¾åƒç‰¹å¾çš„æ–¹æ³•

- åŸºæœ¬æµç¨‹

  å…ˆå¯¹åŸå§‹å›¾åƒè¿›è¡Œç‰¹å¾æå–å†æŠŠæå–åˆ°çš„ç‰¹å¾é€ç»™å…¨è¿æ¥ç½‘ç»œ

  ![image-20210914133938412](img/image-20210914133938412.png)

  - ä¸Šå›¾RBGè¾“å…¥ç‰¹å¾æ˜¯ä¸‰é€šé“
  - è¾“å…¥ç‰¹å¾å›¾çš„æ·±åº¦ï¼ˆchannelæ•°ï¼‰ï¼Œå†³å®šäº†å½“å‰å±‚å·ç§¯æ ¸çš„æ·±åº¦
  - å½“å‰å±‚å·ç§¯æ ¸çš„ä¸ªæ•°ï¼Œå†³å®šäº†å½“å‰å±‚è¾“å‡ºç‰¹å¾å›¾çš„æ·±åº¦



- å·ç§¯æ ¸

  ![image-20210914134449907](img/image-20210914134449907.png)





- å·ç§¯è®¡ç®—

  - æ·±åº¦ä¸º 1 çš„å·ç§¯æ ¸

  ![image-20210914134538610](img/image-20210914134538610.png)

  `(-1)*1+0*0+1*2+(-1)*5+0*4+1*2+(-1)*3+0*4+1*5+1=1`

  

  - æ·±åº¦ä¸º 3 çš„å·ç§¯æ ¸

<img src="img/image-20210914134646997.png" alt="image-20210914134646997" style="zoom: 67%;" />



## æ„Ÿå—é‡

æ„Ÿå—é‡ï¼ˆReceptive Fieldï¼‰ï¼šå·ç§¯ç¥ç»ç½‘ç»œå„è¾“å‡ºç‰¹å¾å›¾ä¸­çš„æ¯ä¸ªåƒç´ ç‚¹ï¼Œåœ¨åŸå§‹è¾“å…¥å›¾ç‰‡ä¸Šæ˜ å°„åŒºåŸŸçš„å¤§å°ã€‚

![image-20210914135238788](img/image-20210914135238788.png)



ç‰¹å¾å›¾è¾ƒå¤§æ—¶ï¼Œä¼˜å…ˆé‡‡ç”¨ä¸¤å±‚ 3 * 3 å·ç§¯æ ¸



## å…¨é›¶å¡«å……ï¼ˆPaddingï¼‰

è¾¹ç¼˜è¡¥0

![image-20210914135548578](img/image-20210914135548578.png)



- TensorFlowå…³é”®å­—

![image-20210914135513465](img/image-20210914135513465.png)



![image-20210914135609163](img/image-20210914135609163.png)



## å·ç§¯å±‚

```python
tf.keras.layers.Conv2D (
    filters = å·ç§¯æ ¸ä¸ªæ•°, 
    kernel_size = å·ç§¯æ ¸å°ºå¯¸,  # æ­£æ–¹å½¢å†™æ ¸é•¿æ•´æ•°ï¼Œæˆ–ï¼ˆæ ¸é«˜hï¼Œæ ¸å®½wï¼‰
    strides = æ»‘åŠ¨æ­¥é•¿,        # æ¨ªçºµå‘ç›¸åŒå†™æ­¥é•¿æ•´æ•°ï¼Œæˆ–(çºµå‘æ­¥é•¿hï¼Œæ¨ªå‘æ­¥é•¿w)ï¼Œé»˜è®¤1
    padding = â€œsameâ€ or â€œvalidâ€,  # ä½¿ç”¨å…¨é›¶å¡«å……æ˜¯â€œsameâ€ï¼Œä¸ä½¿ç”¨æ˜¯â€œvalidâ€ï¼ˆé»˜è®¤ï¼‰
    activation = â€œ reluâ€ or â€œ sigmoid â€ or â€œ tanh â€ or â€œ softmaxâ€ç­‰,  # å¦‚æœ‰BNæ­¤å¤„ä¸å†™
    input_shape = (é«˜, å®½, é€šé“æ•°)  # è¾“å…¥ç‰¹å¾å›¾ç»´åº¦ï¼Œå¯çœç•¥
)

# ä¾‹:
model = tf.keras.models.Sequential([
    Conv2D(6, 5, padding='valid', activation='sigmoid'),
    MaxPool2D(2, 2),
    Conv2D(6, (5, 5), padding='valid', activation='sigmoid'),
    MaxPool2D(2, (2, 2)),
	Conv2D(filters=6, kernel_size=(5, 5),padding='valid', activation='sigmoid'),
    MaxPool2D(pool_size=(2, 2), strides=2),
    Flatten(),
    Dense(10, activation='softmax')
])
```



## æ‰¹æ ‡å‡†åŒ–

> Batch Normalization (BN)ã€€å‰åçš„**æ•°æ®æ ¼å¼ä¸ä¼šå‘ç”Ÿå˜åŒ–**ï¼Œä»…ä»…æ˜¯æ•°å€¼çš„æ ‡å‡†åŒ–å¤„ç†

- æ ‡å‡†åŒ–ï¼šä½¿æ•°æ®ç¬¦åˆ0å‡å€¼ï¼Œ1ä¸ºæ ‡å‡†å·®çš„åˆ†å¸ƒ
- æ‰¹æ ‡å‡†åŒ–ï¼šå¯¹ä¸€å°æ‰¹æ•°æ®ï¼ˆbatchï¼‰ï¼Œåšæ ‡å‡†åŒ–å¤„ç†



- å‡å€¼æ ‡å‡†å·®ï¼šæ±‚è§£(**ä¸€ä¸ªbatchå†…ï¼Œç¬¬kä¸ªå·ç§¯æ ¸ä¸‹ï¼Œè¾“å‡ºç‰¹å¾å›¾ä¸­**) æ‰€æœ‰å…ƒç´ çš„å‡å€¼ï¼Œæ ‡å‡†å·®

![image-20210914140348954](img/image-20210914140348954.png)

![image-20210914140402809](img/image-20210914140402809.png)



- å½’ä¸€åŒ–

  - ä¸€ä¸ªbatchä¸­, å…±æœ‰**batchç»„å›¾**, æ¯ç»„å›¾ç”± **Kä¸ªå­å›¾**ï¼ˆï½†ï½…ï½ï½”ï½•ï½’ï½…ã€€ï½ï½ï½ï¼‰æ„æˆï¼Œå…±ï¼ˆï¼¢ï½ï½”ï½ƒï½ˆï¼Šï¼«ï¼‰å¼ å›¾
  
  - ä¸€ä¸ªå·ç§¯æ ¸ç”Ÿæˆä¸€å¼ å­å›¾ï¼ˆï½†ï½…ï½ï½”ï½•ï½’ï½…ã€€ï½ï½ï½ï¼‰
  
  - å¯¹åŒä¸€å·ç§¯æ ¸ç”Ÿæˆçš„ï½‚ï½ï½”ï½ƒï½ˆå¼ å­å›¾ï¼ˆï½†ï½…ï½ï½”ï½•ï½’ï½…ã€€ï½ï½ï½ï¼‰ä¸­çš„æ‰€æœ‰å…ƒç´ åšå½’ä¸€åŒ–
  
    
  

![image-20210914140511017](img/image-20210914140511017.png)



- ä½œç”¨

  ![image-20210914140921204](img/image-20210914140921204.png)



### ç¼©æ”¾å› å­å’Œåç§»å› å­

>  ä¸ºæ¯ä¸ªå·ç§¯æ ¸å¼•å…¥å¯è®­ç»ƒå‚æ•°ğœ¸å’Œğœ·ï¼Œè°ƒæ•´æ‰¹å½’ä¸€åŒ–çš„åŠ›åº¦

![image-20210914140938416](img/image-20210914140938416.png)



### ä»£ç å®ç°

![image-20210914141154996](img/image-20210914141154996.png)



`tf.keras.layers.BatchNormalization()`

```python
model = tf.keras.models.Sequential([
    Conv2D(filters=6, kernel_size=(5, 5), padding='same'), # å·ç§¯å±‚
    BatchNormalization(), # BNå±‚
    Activation('relu'), # æ¿€æ´»å±‚
    MaxPool2D(pool_size=(2, 2), strides=2, padding='same'), # æ± åŒ–å±‚
    Dropout(0.2), # dropoutå±‚
])
```



## æ± åŒ–

> æ± åŒ–ç”¨äºå‡å°‘ç‰¹å¾æ•°æ®é‡

- æœ€å¤§å€¼æ± åŒ–å¯æå–å›¾ç‰‡çº¹ç†

- å‡å€¼æ± åŒ–å¯ä¿ç•™èƒŒæ™¯ç‰¹å¾

![image-20210914141405881](img/image-20210914141405881.png)



```python
# æœ€å¤§æ± åŒ–
tf.keras.layers.MaxPool2D(
    pool_size=æ± åŒ–æ ¸å°ºå¯¸ï¼Œ#æ­£æ–¹å½¢å†™æ ¸é•¿æ•´æ•°ï¼Œæˆ–ï¼ˆæ ¸é«˜hï¼Œæ ¸å®½wï¼‰
    strides=æ± åŒ–æ­¥é•¿ï¼Œ#æ­¥é•¿æ•´æ•°ï¼Œæˆ–(çºµå‘æ­¥é•¿hï¼Œæ¨ªå‘æ­¥é•¿w)ï¼Œé»˜è®¤ä¸ºpool_size
    padding=â€˜validâ€™orâ€˜sameâ€™#ä½¿ç”¨å…¨é›¶å¡«å……æ˜¯â€œsameâ€ï¼Œä¸ä½¿ç”¨æ˜¯â€œvalidâ€ï¼ˆé»˜è®¤ï¼‰
)

# å‡å€¼æ± åŒ–
tf.keras.layers.AveragePooling2D(
    pool_size=æ± åŒ–æ ¸å°ºå¯¸ï¼Œ#æ­£æ–¹å½¢å†™æ ¸é•¿æ•´æ•°ï¼Œæˆ–ï¼ˆæ ¸é«˜hï¼Œæ ¸å®½wï¼‰
    strides=æ± åŒ–æ­¥é•¿ï¼Œ#æ­¥é•¿æ•´æ•°ï¼Œæˆ–(çºµå‘æ­¥é•¿hï¼Œæ¨ªå‘æ­¥é•¿w)ï¼Œé»˜è®¤ä¸ºpool_size
    padding=â€˜validâ€™orâ€˜sameâ€™#ä½¿ç”¨å…¨é›¶å¡«å……æ˜¯â€œsameâ€ï¼Œä¸ä½¿ç”¨æ˜¯â€œvalidâ€ï¼ˆé»˜è®¤ï¼‰
)

# ä¾‹
model = tf.keras.models.Sequential([
    Conv2D(filters=6, kernel_size=(5, 5), padding='same'), # å·ç§¯å±‚
    BatchNormalization(), # BNå±‚
    Activation('relu'), # æ¿€æ´»å±‚
    MaxPool2D(pool_size=(2, 2), strides=2, padding='same'), # æ± åŒ–å±‚
    Dropout(0.2), # dropoutå±‚
])
```



## èˆå¼ƒ

> åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶ï¼Œå°†ä¸€éƒ¨åˆ†ç¥ç»å…ƒæŒ‰ç…§ä¸€å®šæ¦‚ç‡ä»ç¥ç»ç½‘ç»œä¸­æš‚æ—¶èˆå¼ƒã€‚ç¥ç»ç½‘ç»œä½¿ç”¨æ—¶ï¼Œè¢«èˆå¼ƒçš„ç¥ç»å…ƒä¹Ÿå°†æ¢å¤ã€‚

![image-20210914141813278](img/image-20210914141813278.png)



`tf.keras.layers.Dropout(èˆå¼ƒçš„æ¦‚ç‡)`

```python
model = tf.keras.models.Sequential([
    Conv2D(filters=6, kernel_size=(5, 5), padding='same'), # å·ç§¯å±‚
    BatchNormalization(), # BNå±‚
    Activation('relu'), # æ¿€æ´»å±‚
    MaxPool2D(pool_size=(2, 2), strides=2, padding='same'), # æ± åŒ–å±‚
    Dropout(0.2), # dropoutå±‚
])
```





## å·ç§¯ç½‘ç»œæ­å»ºç¤ºä¾‹



![image-20210923194014740](img/image-20210923194014740.png)



- å·ç§¯ç½‘ç»œç»“æ„

  ```python
  class Baseline(Model):
      def __init__(self):
          super(Baseline, self).__init__()
          self.c1 = Conv2D(filters=6, kernel_size=(5, 5), padding='same')   # C: å·ç§¯å±‚
          self.b1 = BatchNormalization()  								  # B: BNå±‚
          self.a1 = Activation('relu')  									  # A: æ¿€æ´»å±‚
          self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')  # P: æ± åŒ–å±‚
          self.d1 = Dropout(0.2)  										  # D: dropoutå±‚
  
          self.flatten = Flatten() # æ‹‰ç›´
          
          self.f1 = Dense(128, activation='relu')
          self.d2 = Dropout(0.2)
          self.f2 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.c1(x)
          x = self.b1(x)
          x = self.a1(x)
          x = self.p1(x)
          x = self.d1(x)
  
          x = self.flatten(x)
          x = self.f1(x)
          x = self.d2(x)
          y = self.f2(x)
          return y
  ```

  

- Baseline

  ```python
  import tensorflow as tf
  import os
  import numpy as np
  from matplotlib import pyplot as plt
  from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense
  from tensorflow.keras import Model
  
  np.set_printoptions(threshold=np.inf)
  
  cifar10 = tf.keras.datasets.cifar10
  (x_train, y_train), (x_test, y_test) = cifar10.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  
  class Baseline(Model):
      def __init__(self):
          super(Baseline, self).__init__()
          self.c1 = Conv2D(filters=6, kernel_size=(5, 5), padding='same')  # å·ç§¯å±‚
          self.b1 = BatchNormalization()  # BNå±‚
          self.a1 = Activation('relu')  # æ¿€æ´»å±‚
          self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')  # æ± åŒ–å±‚
          self.d1 = Dropout(0.2)  # dropoutå±‚
  
          self.flatten = Flatten()
          self.f1 = Dense(128, activation='relu')
          self.d2 = Dropout(0.2)
          self.f2 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.c1(x)
          x = self.b1(x)
          x = self.a1(x)
          x = self.p1(x)
          x = self.d1(x)
  
          x = self.flatten(x)
          x = self.f1(x)
          x = self.d2(x)
          y = self.f2(x)
          return y
  
  
  model = Baseline()
  
  model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['sparse_categorical_accuracy'])
  
  checkpoint_save_path = "./checkpoint/Baseline.ckpt"
  if os.path.exists(checkpoint_save_path + '.index'):
      print('-------------load the model-----------------')
      model.load_weights(checkpoint_save_path)
  
  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                   save_weights_only=True,
                                                   save_best_only=True)
  
  history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback])
  model.summary()
  
  # print(model.trainable_variables)
  file = open('./weights.txt', 'w')
  for v in model.trainable_variables:
      file.write(str(v.name) + '\n')
      file.write(str(v.shape) + '\n')
      file.write(str(v.numpy()) + '\n')
  file.close()
  
  ############################    show   ################################
  
  # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„accå’Œlossæ›²çº¿
  acc = history.history['sparse_categorical_accuracy']
  val_acc = history.history['val_sparse_categorical_accuracy']
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  
  plt.subplot(1, 2, 1)
  plt.plot(acc, label='Training Accuracy')
  plt.plot(val_acc, label='Validation Accuracy')
  plt.title('Training and Validation Accuracy')
  plt.legend()
  
  plt.subplot(1, 2, 2)
  plt.plot(loss, label='Training Loss')
  plt.plot(val_loss, label='Validation Loss')
  plt.title('Training and Validation Loss')
  plt.legend()
  plt.show()
  ```



## LeNet

> LeNetç”±Yann LeCunäº1998å¹´æå‡ºï¼Œå·ç§¯ç½‘ç»œå¼€ç¯‡ä¹‹ä½œã€‚
> Yann Lecun, Leon Bottou, Y. Bengio, Patrick Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 1998.



- å·ç§¯ç½‘ç»œç»“æ„

  ![image-20210923194811156](img/image-20210923194811156.png)



- ä»£ç 

  ![image-20210923195114222](img/image-20210923195114222.png)

  ```python
  class LeNet5(Model):
      def __init__(self):
          super(LeNet5, self).__init__()
          self.c1 = Conv2D(filters=6, kernel_size=(5, 5), activation='sigmoid')
          self.p1 = MaxPool2D(pool_size=(2, 2), strides=2)
  
          self.c2 = Conv2D(filters=16, kernel_size=(5, 5), activation='sigmoid')
          self.p2 = MaxPool2D(pool_size=(2, 2), strides=2)
  
          self.flatten = Flatten()
          self.f1 = Dense(120, activation='sigmoid')
          self.f2 = Dense(84, activation='sigmoid')
          self.f3 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.c1(x)
          x = self.p1(x)
  
          x = self.c2(x)
          x = self.p2(x)
  
          x = self.flatten(x)
          x = self.f1(x)
          x = self.f2(x)
          y = self.f3(x)
          return y
  ```

  

## AlexNet

> AlexNetç½‘ç»œè¯ç”Ÿäº2012å¹´ï¼Œå½“å¹´ImageNetç«èµ›çš„å† å†›ï¼ŒTop5é”™è¯¯ç‡ä¸º16.4%
> Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In NIPS, 2012.



- å·ç§¯ç½‘ç»œç»“æ„

  ![image-20210923195341412](img/image-20210923195341412.png)



- ä»£ç 

  ![image-20210923195604450](img/image-20210923195604450.png)
  
  ```python
  class AlexNet8(Model):
      def __init__(self):
          super(AlexNet8, self).__init__()
          self.c1 = Conv2D(filters=96, kernel_size=(3, 3))
          self.b1 = BatchNormalization()
          self.a1 = Activation('relu')
          self.p1 = MaxPool2D(pool_size=(3, 3), strides=2)
  
          self.c2 = Conv2D(filters=256, kernel_size=(3, 3))
          self.b2 = BatchNormalization()
          self.a2 = Activation('relu')
          self.p2 = MaxPool2D(pool_size=(3, 3), strides=2)
  
          self.c3 = Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu')
                           
          self.c4 = Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu')
                           
          self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')
          self.p3 = MaxPool2D(pool_size=(3, 3), strides=2)
  
          self.flatten = Flatten()
          self.f1 = Dense(2048, activation='relu')
          self.d1 = Dropout(0.5)
          self.f2 = Dense(2048, activation='relu')
          self.d2 = Dropout(0.5)
          self.f3 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.c1(x)
          x = self.b1(x)
          x = self.a1(x)
          x = self.p1(x)
  
          x = self.c2(x)
          x = self.b2(x)
          x = self.a2(x)
          x = self.p2(x)
  
          x = self.c3(x)
  
          x = self.c4(x)
  
          x = self.c5(x)
          x = self.p3(x)
  
          x = self.flatten(x)
          x = self.f1(x)
          x = self.d1(x)
          x = self.f2(x)
          x = self.d2(x)
          y = self.f3(x)
          return y
  ```
  
  

## VGGNet

> VGGNetè¯ç”Ÿäº2014å¹´ï¼Œå½“å¹´ImageNetç«èµ›çš„äºšå†›ï¼ŒTop5é”™è¯¯ç‡å‡å°åˆ°7.3%
> K. Simonyan, A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition.In ICLR,
> 2015.



- ä»£ç 

  ![image-20210923200208773](img/image-20210923200208773.png)

  ```python
  class VGG16(Model):
      def __init__(self):
          super(VGG16, self).__init__()
          self.c1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')  # å·ç§¯å±‚1
          self.b1 = BatchNormalization()  # BNå±‚1
          self.a1 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same', )
          self.b2 = BatchNormalization()  # BNå±‚1
          self.a2 = Activation('relu')  # æ¿€æ´»å±‚1
          self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')
          self.d1 = Dropout(0.2)  # dropoutå±‚
  
          self.c3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')
          self.b3 = BatchNormalization()  # BNå±‚1
          self.a3 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c4 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')
          self.b4 = BatchNormalization()  # BNå±‚1
          self.a4 = Activation('relu')  # æ¿€æ´»å±‚1
          self.p2 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')
          self.d2 = Dropout(0.2)  # dropoutå±‚
  
          self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
          self.b5 = BatchNormalization()  # BNå±‚1
          self.a5 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c6 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
          self.b6 = BatchNormalization()  # BNå±‚1
          self.a6 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c7 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
          self.b7 = BatchNormalization()
          self.a7 = Activation('relu')
          self.p3 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')
          self.d3 = Dropout(0.2)
  
          self.c8 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
          self.b8 = BatchNormalization()  # BNå±‚1
          self.a8 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c9 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
          self.b9 = BatchNormalization()  # BNå±‚1
          self.a9 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c10 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
          self.b10 = BatchNormalization()
          self.a10 = Activation('relu')
          self.p4 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')
          self.d4 = Dropout(0.2)
  
          self.c11 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
          self.b11 = BatchNormalization()  # BNå±‚1
          self.a11 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c12 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
          self.b12 = BatchNormalization()  # BNå±‚1
          self.a12 = Activation('relu')  # æ¿€æ´»å±‚1
          self.c13 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
          self.b13 = BatchNormalization()
          self.a13 = Activation('relu')
          self.p5 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')
          self.d5 = Dropout(0.2)
  
          self.flatten = Flatten()
          self.f1 = Dense(512, activation='relu')
          self.d6 = Dropout(0.2)
          self.f2 = Dense(512, activation='relu')
          self.d7 = Dropout(0.2)
          self.f3 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.c1(x)
          x = self.b1(x)
          x = self.a1(x)
          x = self.c2(x)
          x = self.b2(x)
          x = self.a2(x)
          x = self.p1(x)
          x = self.d1(x)
  
          x = self.c3(x)
          x = self.b3(x)
          x = self.a3(x)
          x = self.c4(x)
          x = self.b4(x)
          x = self.a4(x)
          x = self.p2(x)
          x = self.d2(x)
  
          x = self.c5(x)
          x = self.b5(x)
          x = self.a5(x)
          x = self.c6(x)
          x = self.b6(x)
          x = self.a6(x)
          x = self.c7(x)
          x = self.b7(x)
          x = self.a7(x)
          x = self.p3(x)
          x = self.d3(x)
  
          x = self.c8(x)
          x = self.b8(x)
          x = self.a8(x)
          x = self.c9(x)
          x = self.b9(x)
          x = self.a9(x)
          x = self.c10(x)
          x = self.b10(x)
          x = self.a10(x)
          x = self.p4(x)
          x = self.d4(x)
  
          x = self.c11(x)
          x = self.b11(x)
          x = self.a11(x)
          x = self.c12(x)
          x = self.b12(x)
          x = self.a12(x)
          x = self.c13(x)
          x = self.b13(x)
          x = self.a13(x)
          x = self.p5(x)
          x = self.d5(x)
  
          x = self.flatten(x)
          x = self.f1(x)
          x = self.d6(x)
          x = self.f2(x)
          x = self.d7(x)
          y = self.f3(x)
          return y
  ```

  

## InceptionNet

> InceptionNetè¯ç”Ÿäº2014å¹´ï¼Œå½“å¹´ImageNetç«èµ›å† å†›ï¼ŒTop5é”™è¯¯ç‡ä¸º6.67%
> Szegedy C, Liu W, Jia Y, et al. Going Deeper with Convolutions. In CVPR, 2015.



- å·ç§¯ç½‘ç»œç»“æ„

  - å•ä¸ªblock

  ![image-20210923200500732](img/image-20210923200500732.png)

  

  - å®Œæ•´ç»“æ„
    - é»„è‰²æ¡†å›¾ç”±4ä¸ªblockç»„æˆ

â€‹	<img src="img/image-20210923201133948.png" alt="image-20210923201133948" style="zoom:120%;" />



- ä»£ç 

  - æœ€å°å•å…ƒ

      ```python
      class ConvBNRelu(Model):
          def __init__(self, ch, kernelsz=3, strides=1, padding='same'):
              super(ConvBNRelu, self).__init__()
              self.model = tf.keras.models.Sequential([
                  Conv2D(ch, kernelsz, strides=strides, padding=padding),
                  BatchNormalization(),
                  Activation('relu')
              ])

          def call(self, x):
              x = self.model(x, training=False) 
              # training=Falseæ—¶ï¼ŒBNé€šè¿‡æ•´ä¸ªè®­ç»ƒé›†è®¡ç®—å‡å€¼ã€æ–¹å·®å»åšæ‰¹å½’ä¸€åŒ–ï¼Œ
              # training=Trueæ—¶ï¼Œé€šè¿‡å½“å‰batchçš„å‡å€¼ã€æ–¹å·®å»åšæ‰¹å½’ä¸€åŒ–ã€‚æ¨ç†æ—¶ training=Falseæ•ˆæœå¥½
              return x
      ```

  - block

    ```python
    class InceptionBlk(Model):
        def __init__(self, ch, strides=1):
            super(InceptionBlk, self).__init__()
            self.ch = ch
            self.strides = strides
            self.c1 = ConvBNRelu(ch, kernelsz=1, strides=strides)
            self.c2_1 = ConvBNRelu(ch, kernelsz=1, strides=strides)
            self.c2_2 = ConvBNRelu(ch, kernelsz=3, strides=1)
            self.c3_1 = ConvBNRelu(ch, kernelsz=1, strides=strides)
            self.c3_2 = ConvBNRelu(ch, kernelsz=5, strides=1)
            self.p4_1 = MaxPool2D(3, strides=1, padding='same')
            self.c4_2 = ConvBNRelu(ch, kernelsz=1, strides=strides)
    
        def call(self, x):
            x1 = self.c1(x)
            x2_1 = self.c2_1(x)
            x2_2 = self.c2_2(x2_1)
            x3_1 = self.c3_1(x)
            x3_2 = self.c3_2(x3_1)
            x4_1 = self.p4_1(x)
            x4_2 = self.c4_2(x4_1)
    
            x = tf.concat([x1, x2_2, x3_2, x4_2], axis=3) # concat along axis=channel
            return x
    ```

  - å®Œæ•´ç»“æ„

      ```python
      class Inception10(Model):
          def __init__(self, num_blocks, num_classes, init_ch=16, **kwargs):
              super(Inception10, self).__init__(**kwargs)
              self.in_channels = init_ch
              self.out_channels = init_ch
              self.num_blocks = num_blocks
              self.init_ch = init_ch
              self.c1 = ConvBNRelu(init_ch)
              self.blocks = tf.keras.models.Sequential()
              for block_id in range(num_blocks):
                  for layer_id in range(2):
                      if layer_id == 0:
                          block = InceptionBlk(self.out_channels, strides=2)
                      else:
                          block = InceptionBlk(self.out_channels, strides=1)
                      self.blocks.add(block)
                  # enlarger out_channels per block
                  self.out_channels *= 2
              self.p1 = GlobalAveragePooling2D()
              self.f1 = Dense(num_classes, activation='softmax')
      
          def call(self, x):
              x = self.c1(x)
              x = self.blocks(x)
              x = self.p1(x)
              y = self.f1(x)
              return y
      
      model = Inception10(num_blocks=2, num_classes=10)
      ```

      







## ResNet

> ResNetè¯ç”Ÿäº2015å¹´ï¼Œå½“å¹´ImageNetç«èµ›å† å†›ï¼ŒTop5é”™è¯¯ç‡ä¸º3.57%
> Kaiming He, Xiangyu Zhang, Shaoqing Ren. Deep Residual Learning for Image Recognition. In CPVR,
> 2016.



- å·ç§¯ç½‘ç»œç»“æ„

  ![image-20210923202253484](img/image-20210923202253484.png)



â€‹	![image-20210923202239299](img/image-20210923202239299.png)





- ä»£ç ç»“æ„

  <img src="img/image-20210923202428767.png" alt="image-20210923202428767" style="zoom:100%;" />



â€‹	![image-20210923202600001](img/image-20210923202600001.png)



- ResNetå—

```python
class ResnetBlock(Model):

    def __init__(self, filters, strides=1, residual_path=False):
        super(ResnetBlock, self).__init__()
        self.filters = filters
        self.strides = strides
        self.residual_path = residual_path

        self.c1 = Conv2D(filters, (3, 3), strides=strides, padding='same', use_bias=False)
        self.b1 = BatchNormalization()
        self.a1 = Activation('relu')

        self.c2 = Conv2D(filters, (3, 3), strides=1, padding='same', use_bias=False)
        self.b2 = BatchNormalization()

        # residual_pathä¸ºTrueæ—¶ï¼Œå¯¹è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·ï¼Œå³ç”¨1x1çš„å·ç§¯æ ¸åšå·ç§¯æ“ä½œï¼Œä¿è¯xèƒ½å’ŒF(x)ç»´åº¦ç›¸åŒï¼Œé¡ºåˆ©ç›¸åŠ 
        if residual_path:
            self.down_c1 = Conv2D(filters, (1, 1), strides=strides, padding='same', use_bias=False)
            self.down_b1 = BatchNormalization()
        
        self.a2 = Activation('relu')

    def call(self, inputs):
        residual = inputs  # residualç­‰äºè¾“å…¥å€¼æœ¬èº«ï¼Œå³residual=x
        # å°†è¾“å…¥é€šè¿‡å·ç§¯ã€BNå±‚ã€æ¿€æ´»å±‚ï¼Œè®¡ç®—F(x)
        x = self.c1(inputs)
        x = self.b1(x)
        x = self.a1(x)

        x = self.c2(x)
        y = self.b2(x)

        if self.residual_path:
            residual = self.down_c1(inputs)
            residual = self.down_b1(residual)

        out = self.a2(y + residual)  # æœ€åè¾“å‡ºçš„æ˜¯ä¸¤éƒ¨åˆ†çš„å’Œï¼Œå³F(x)+xæˆ–F(x)+Wx,å†è¿‡æ¿€æ´»å‡½æ•°
        return out
```



- æ•´ä½“ç»“æ„

```python
class ResNet18(Model):

    def __init__(self, block_list, initial_filters=64):  # block_listè¡¨ç¤ºæ¯ä¸ªblockæœ‰å‡ ä¸ªå·ç§¯å±‚
        super(ResNet18, self).__init__()
        self.num_blocks = len(block_list)  # å…±æœ‰å‡ ä¸ªblock
        self.block_list = block_list
        self.out_filters = initial_filters
        self.c1 = Conv2D(self.out_filters, (3, 3), strides=1, padding='same', use_bias=False)
        self.b1 = BatchNormalization()
        self.a1 = Activation('relu')
        self.blocks = tf.keras.models.Sequential()
        # æ„å»ºResNetç½‘ç»œç»“æ„
        for block_id in range(len(block_list)):  # ç¬¬å‡ ä¸ªresnet block
            for layer_id in range(block_list[block_id]):  # ç¬¬å‡ ä¸ªå·ç§¯å±‚
                
                if block_id != 0 and layer_id == 0:  # å¯¹é™¤ç¬¬ä¸€ä¸ªblockä»¥å¤–çš„æ¯ä¸ªblockçš„è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·
                    block = ResnetBlock(self.out_filters, strides=2, residual_path=True)
                else:
                    block = ResnetBlock(self.out_filters, residual_path=False)
                self.blocks.add(block)  # å°†æ„å»ºå¥½çš„blockåŠ å…¥resnet
                
            self.out_filters *= 2  # ä¸‹ä¸€ä¸ªblockçš„å·ç§¯æ ¸æ•°æ˜¯ä¸Šä¸€ä¸ªblockçš„2å€
            
        self.p1 = tf.keras.layers.GlobalAveragePooling2D()
        self.f1 = tf.keras.layers.Dense(10, activation='softmax', 			 							kernel_regularizer=tf.keras.regularizers.l2())

    def call(self, inputs):
        x = self.c1(inputs)
        x = self.b1(x)
        x = self.a1(x)
        x = self.blocks(x)
        x = self.p1(x)
        y = self.f1(x)
        return y
```





## ç»å…¸å·ç§¯ç½‘ç»œ

![image-20210923203806642](img/image-20210923203806642.png)





# å¾ªç¯ç¥ç»ç½‘ç»œ



## å¾ªç¯æ ¸

- å‰å‘ä¼ æ’­æ—¶ï¼š
  - è®°å¿†ä½“å†…å­˜å‚¨çš„**çŠ¶æ€ä¿¡æ¯htï¼Œåœ¨æ¯ä¸ªæ—¶åˆ»éƒ½è¢«åˆ·æ–°**
  - ä¸‰ä¸ªå‚æ•°çŸ©é˜µ**wxhï¼Œwhhï¼Œwhyè‡ªå§‹è‡³ç»ˆéƒ½æ˜¯å›ºå®šä¸å˜**
- åå‘ä¼ æ’­æ—¶ï¼š
  - ä¸‰ä¸ªå‚æ•°çŸ©é˜µ**wxhï¼Œwhhï¼Œwhyè¢«æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°**

![image-20210923204834386](img/image-20210923204834386.png)

![image-20210925171201900](img/image-20210925171201900.png)

![image-20210923205034365](img/image-20210923205034365.png)



## å¾ªç¯è®¡ç®—å±‚

> å‘è¾“å‡ºæ–¹å‘ç”Ÿé•¿, ä¸æ–­å åŠ 



![image-20210925171328635](img/image-20210925171328635.png)



### `SimpleRNN`

ä¸€ä¸ªå¾ªç¯æ ¸å¯ä»¥æœ‰å¤šä¸ªè®°å¿†ä½“

å»ºç«‹ä¸€ä¸ª`SimpleRNN`å¯¹åº”å»ºç«‹ä¸€ä¸ªå¾ªç¯è®¡ç®—å±‚, `SimpleRNN`å¯å åŠ , ç”Ÿæˆå¤šå±‚å¾ªç¯è®¡ç®—å±‚

```python
tf.keras.layers.SimpleRNN(è®°å¿†ä½“ä¸ªæ•°ï¼Œactivation=â€˜æ¿€æ´»å‡½æ•°â€™ï¼Œreturn_sequences=æ˜¯å¦æ¯ä¸ªæ—¶åˆ»è¾“å‡ºhtåˆ°ä¸‹ä¸€å±‚)

# è®°å¿†ä½“ä¸ªæ•°ï¼šå³å¾ªç¯æ ¸ä¸­è®°å¿†ä½“çš„ä¸ªæ•°
# activation=â€˜æ¿€æ´»å‡½æ•°â€™   ä¸å†™ï¼Œé»˜è®¤ä½¿ç”¨tanh
# return_sequences=True  å„æ—¶é—´æ­¥è¾“å‡ºht
# return_sequences=False ä»…æœ€åæ—¶é—´æ­¥è¾“å‡ºhtï¼ˆé»˜è®¤ï¼‰

ä¾‹ï¼šSimpleRNN(3, return_sequences=True)

```



- return_sequences

  å½“ä¸‹ä¸€å±‚ä¾ç„¶æ˜¯RNNå±‚ï¼Œé€šå¸¸ä¸ºTrue; åä¹‹å¦‚æœåé¢æ˜¯Denseå±‚ï¼Œé€šå¸¸ä¸ºFasle

  - `return_sequences=True`

  ![image-20210925171927616](img/image-20210925171927616.png)

  - `return_sequences=False`

    ![image-20210925172026160](img/image-20210925172026160.png)





## è¾“å…¥æ ¼å¼

- x_trainç»´åº¦ï¼š
  **[æ ·æœ¬æ•°é‡(æœ‰å‡ å¥è¯)ï¼Œæ—¶é—´æ­¥æ•°(æœ‰å‡ ä¸ªæ—¶é—´æ­¥)ï¼Œæ¯ä¸ªæ—¶é—´æ­¥è¾“å…¥ç‰¹å¾ä¸ªæ•°(ä¸€ä¸ªå•è¯ç”±å‡ ç»´å‘é‡è¡¨ç¤º)]**

  ![image-20210925172625056](img/image-20210925172625056.png)

  

## å•å­—æ¯é¢„æµ‹

- ä»¥å­—æ¯é¢„æµ‹ä¸ºä¾‹ï¼šè¾“å…¥aé¢„æµ‹å‡ºbï¼Œè¾“å…¥bé¢„æµ‹å‡ºcï¼Œè¾“å…¥cé¢„æµ‹å‡ºdï¼Œè¾“å…¥dé¢„æµ‹å‡ºeï¼Œè¾“å…¥eé¢„æµ‹å‡ºa

- å­—æ¯ç¼–ç (å‘é‡è¡¨ç¤º)

![image-20210925172734542](img/image-20210925172734542.png)

- å‰å‘ä¼ æ’­è®¡ç®—æ–¹å¼

<img src="img/image-20210925172745975.png" alt="image-20210925172745975" style="zoom:125%;" />

![image-20210925172951309](img/image-20210925172951309.png)

![image-20210925173554693](img/image-20210925173554693.png)



- ä»£ç 

  - æ•°æ®å‡†å¤‡

    ```python
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, SimpleRNN
    import matplotlib.pyplot as plt
    import os
    
    input_word = "abcde"
    w_to_id = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}  # å•è¯æ˜ å°„åˆ°æ•°å€¼idçš„è¯å…¸
    id_to_onehot = {0: [1., 0., 0., 0., 0.], 
                    1: [0., 1., 0., 0., 0.], 
                    2: [0., 0., 1., 0., 0.], 
                    3: [0., 0., 0., 1., 0.], 
                    4: [0., 0., 0., 0., 1.]}  # idç¼–ç ä¸ºone-hot
    
    # å°†å­—æ¯è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
    x_train = [id_to_onehot[w_to_id['a']], 
               id_to_onehot[w_to_id['b']], 
               id_to_onehot[w_to_id['c']],
               id_to_onehot[w_to_id['d']], 
               id_to_onehot[w_to_id['e']]]
    y_train = [w_to_id['b'], w_to_id['c'], w_to_id['d'], w_to_id['e'], w_to_id['a']]
    
    # æ‰“ä¹±æ•°æ®é›†é¡ºåº
    np.random.seed(7)
    np.random.shuffle(x_train)
    np.random.seed(7)
    np.random.shuffle(y_train)
    tf.random.set_seed(7)
    
    # ä½¿x_trainç¬¦åˆSimpleRNNè¾“å…¥è¦æ±‚ï¼š[é€å…¥æ ·æœ¬æ•°ï¼Œ å¾ªç¯æ ¸æ—¶é—´å±•å¼€æ­¥æ•°ï¼Œ æ¯ä¸ªæ—¶é—´æ­¥è¾“å…¥ç‰¹å¾ä¸ªæ•°]ã€‚
    # æ­¤å¤„æ•´ä¸ªæ•°æ®é›†é€å…¥ï¼Œé€å…¥æ ·æœ¬æ•°ä¸ºlen(x_train)
    # è¾“å…¥1ä¸ªå­—æ¯å‡ºç»“æœï¼Œå¾ªç¯æ ¸æ—¶é—´å±•å¼€æ­¥æ•°ä¸º1
    # è¡¨ç¤ºä¸ºç‹¬çƒ­ç æœ‰5ä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¯ä¸ªæ—¶é—´æ­¥è¾“å…¥ç‰¹å¾ä¸ªæ•°ä¸º5
    x_train = np.reshape(x_train, (len(x_train), 1, 5))
    y_train = np.array(y_train)
    ```

    

  - æ¨¡å‹è®­ç»ƒ

    ```python
    model = tf.keras.Sequential([
        SimpleRNN(3),
        Dense(5, activation='softmax')
    ])
    
    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['sparse_categorical_accuracy'])
    
    checkpoint_save_path = "./checkpoint/rnn_onehot_1pre1.ckpt"
    
    if os.path.exists(checkpoint_save_path + '.index'):
        print('-------------load the model-----------------')
        model.load_weights(checkpoint_save_path)
    
    cp_callback = tf.keras.callbacks.ModelCheckpoint(
        				filepath=checkpoint_save_path,
                        save_weights_only=True,
                        save_best_only=True,
                        monitor='loss')  # ç”±äºfitæ²¡æœ‰ç»™å‡ºæµ‹è¯•é›†ï¼Œä¸è®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡ï¼Œæ ¹æ®lossï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹
    
    history = model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cp_callback])
    
    model.summary()
    
    # print(model.trainable_variables)
    file = open('./weights.txt', 'w')  # å‚æ•°æå–
    for v in model.trainable_variables:
        file.write(str(v.name) + '\n')
        file.write(str(v.shape) + '\n')
        file.write(str(v.numpy()) + '\n')
    file.close()
    ```

    

  - df

    





## å¤šå­—æ¯é¢„æµ‹







## Embedding





## è‚¡ç¥¨é¢„æµ‹



### RNN



### LSTM



### GRU





