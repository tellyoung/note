# conda

```shell
conda create -n TF2.1 python=3.7

conda activate TF2.1

conda install cudatoolkit=10.1

conda install cudnn=7.6

pip install tensorflow==2.1

===éªŒè¯è¿›å…¥pythonç¯å¢ƒ===
>>>import tensorflow as tf
>>>tf.__version__
```



# å¸¸ç”¨æ•°æ®é›†

## IRIS







## MNIST

![image-20210913184459039](img/image-20210913184459039.png)



- å¯¼å…¥æ•°æ®

  ```python
  import tensorflow as tf
  
  mnist = tf.keras.datasets.mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  ```



- æ•°æ®å¯è§†åŒ–
    ```python
    import tensorflow as tf
    from matplotlib import pyplot as plt

    # å¯¼å…¥æ•°æ®é›†
    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    # ==================å¯è§†åŒ–======================
    # å¯è§†åŒ–è®­ç»ƒé›†è¾“å…¥ç‰¹å¾çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
    plt.imshow(x_train[0], cmap='gray')  # ç»˜åˆ¶ç°åº¦å›¾
    plt.show()

    # æ‰“å°å‡ºè®­ç»ƒé›†è¾“å…¥ç‰¹å¾çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
    print("x_train[0]:\n", x_train[0])

    # æ‰“å°å‡ºè®­ç»ƒé›†æ ‡ç­¾çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
    print("y_train[0]:\n", y_train[0])

    # æ‰“å°å‡ºæ•´ä¸ªè®­ç»ƒé›†è¾“å…¥ç‰¹å¾å½¢çŠ¶
    print("x_train.shape:\n", x_train.shape)

    # æ‰“å°å‡ºæ•´ä¸ªè®­ç»ƒé›†æ ‡ç­¾çš„å½¢çŠ¶
    print("y_train.shape:\n", y_train.shape)
    # ===============================================
    ```



## FASHION

![image-20210913185818337](img/image-20210913185818337.png)



- å¯¼å…¥æ•°æ®

  ```python
  import tensorflow as tf
  
  fashion = tf.keras.datasets.fashion_mnist
  (x_train, y_train),(x_test, y_test) = fashion.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  ```

  

## Cifar10







# TensorFlow2.1

![image-20210910151051087](img/image-20210910151051087.png)



## å¸¸ç”¨æ–¹æ³•

### å¼ é‡

<img src="img/image-20210909150959178.png" alt="image-20210909150959178" style="zoom:67%;" />

![image-20210909151544109](img/image-20210909151544109.png)





- `tf.constant(å¼ é‡å†…å®¹ï¼Œdtype=æ•°æ®ç±»å‹(å¯é€‰))`

  ```python
  a = tf.constant([1, 5], dtype=tf.int64)
  print(a)
  print(a.dtype)
  print(a.shape)
  ```

  

- `tf.convert_to_tensor(æ•°æ®åï¼Œdtype=æ•°æ®ç±»å‹(å¯é€‰))`

  å°†numpyçš„æ•°æ®ç±»å‹è½¬æ¢ä¸ºTensoræ•°æ®ç±»å‹

  ```python
  import numpy as np
  a = np.arange(0, 5)
  b = tf.convert_to_tensor(a, dtype=tf.int64)
  print(a)
  print(b)
  ```

  

- `tf.zeros(ç»´åº¦)`

  åˆ›å»ºå…¨ä¸º0çš„å¼ é‡



- `tf.ones(ç»´åº¦)`

  åˆ›å»ºå…¨ä¸º1çš„å¼ é‡



- `tf.fill(ç»´åº¦ï¼ŒæŒ‡å®šå€¼)`

  åˆ›å»ºå…¨ä¸ºæŒ‡å®šå€¼çš„å¼ é‡



- ç»´åº¦ï¼š
  - ä¸€ç»´ç›´æ¥å†™ä¸ªæ•°
  - äºŒç»´ç”¨[è¡Œï¼Œåˆ—]
  - å¤šç»´ç”¨[n,m,j,kâ€¦â€¦]

```python
a = tf.zeros([2, 3])
b = tf.ones(4)
c = tf.fill([2, 2], 9)
print(a)
print(b)
print(c)
```



- np.random.RandomState.rand()
  è¿”å›ä¸€ä¸ª[0,1)ä¹‹é—´çš„éšæœºæ•°
  `np.random.RandomState.rand(ç»´åº¦)`

  ```python
  import numpy as np
  rdm=np.random.RandomState(seed=1) # seed=å¸¸æ•°æ¯æ¬¡ç”Ÿæˆéšæœºæ•°ç›¸åŒ
  a=rdm.rand()     # è¿”å›ä¸€ä¸ªéšæœºæ ‡é‡
  b=rdm.rand(2, 3) # è¿”å›ç»´åº¦ä¸º2è¡Œ3åˆ—éšæœºæ•°çŸ©é˜µ
  print("a:", a)
  print("b:", b)
  
  # a: 0.417022004702574
  # b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01]
  # 	  [1.46755891e-01 9.23385948e-02 1.86260211e-01]]
  ```

  

- `tf.random.normal(ç»´åº¦ï¼Œmean=å‡å€¼ï¼Œstddev=æ ‡å‡†å·®)`

  ç”Ÿæˆæ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œé»˜è®¤å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1



- `tf.random.truncated_normal (ç»´åº¦ï¼Œmean=å‡å€¼ï¼Œstddev=æ ‡å‡†å·®)`

  ç”Ÿæˆæˆªæ–­å¼æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œåœ¨`tf.truncated_normal`ä¸­å¦‚æœéšæœºç”Ÿæˆæ•°æ®çš„å–å€¼åœ¨ï¼ˆÎ¼-2Ïƒï¼ŒÎ¼+2Ïƒï¼‰ä¹‹å¤–åˆ™é‡æ–°è¿›è¡Œç”Ÿæˆï¼Œä¿è¯äº†ç”Ÿæˆå€¼åœ¨å‡å€¼é™„è¿‘

```python
d = tf.random.normal([2, 2], mean=5, stddev=1)
print(d)
e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=10)
print(e)
```



- `tf.random.uniform(ç»´åº¦ï¼Œminval=æœ€å°å€¼ï¼Œmaxval=æœ€å¤§å€¼)`

  ç”Ÿæˆå‡åŒ€åˆ†å¸ƒéšæœºæ•°[minval, maxval)

  ```python
  f = tf.random.uniform([5, 5], minval=0, maxval=1)
  print(f)
  ```



- `tf.cast(å¼ é‡åï¼Œdtype=æ•°æ®ç±»å‹)`

  å¼ºåˆ¶tensorè½¬æ¢ä¸ºè¯¥æ•°æ®ç±»å‹



- `tf.reduce_min(å¼ é‡å)`

  è®¡ç®—å¼ é‡ç»´åº¦ä¸Šå…ƒç´ çš„æœ€å°å€¼

- `tf.reduce_max(å¼ é‡å)`

  è®¡ç®—å¼ é‡ç»´åº¦ä¸Šå…ƒç´ çš„æœ€å¤§å€¼

```python
x1 = tf.constant([1., 2., 3.],dtype=tf.float64)
print(x1)

x2 = tf.cast(x1, tf.int32)
print(x2)
print (tf.reduce_min(x2), tf.reduce_max(x2))
```



- `tf.reduce_mean(å¼ é‡åï¼Œaxis=æ“ä½œè½´)`

  è®¡ç®—å¼ é‡æ²¿ç€æŒ‡å®šç»´åº¦çš„å¹³å‡å€¼

- `tf.reduce_sum(å¼ é‡åï¼Œaxis=æ“ä½œè½´)`

  è®¡ç®—å¼ é‡æ²¿ç€æŒ‡å®šç»´åº¦çš„å’Œ

```python
x=tf.constant([[1, 2, 3],
               [3, 2, 3]])
print(x)
print(tf.reduce_mean(x)) # æ‰€æœ‰æ•°å€¼çš„å‡å€¼
print(tf.reduce_mean(x, axis=0))
print(tf.reduce_sum(x, axis=1)) # æ‰€æœ‰æ•°å€¼çš„å’Œ >>>14
```



- axis

![image-20210909153000311](img/image-20210909153000311.png)



#### æ“ä½œå¼ é‡

- `tf.where()`
  æ¡ä»¶è¯­å¥çœŸè¿”å›Aï¼Œæ¡ä»¶è¯­å¥å‡è¿”å›B (å¼ é‡å†…éƒ¨å…ƒç´ å±‚é¢)
  `tf.where(æ¡ä»¶è¯­å¥ï¼Œ çœŸè¿”å›Aï¼Œ å‡è¿”å›B)`

  ```python
  a=tf.constant([1,2,3,1,1])
  b=tf.constant([0,1,3,4,5])
  c=tf.where(tf.greater(a, b), a, b) # è‹¥a>bï¼Œè¿”å›aå¯¹åº”ä½ç½®çš„å…ƒç´ ï¼Œå¦åˆ™è¿”å›bå¯¹åº”ä½ç½®çš„å…ƒç´ 
  print("c:", c)
  
  # cï¼štf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)
  ```



- `np.vstack()`

  å°†ä¸¤ä¸ªæ•°ç»„æŒ‰å‚ç›´æ–¹å‘å åŠ 
  `np.vstack(æ•°ç»„1ï¼Œæ•°ç»„2)`

  ```python
  import numpy as np
  a = np.array([1,2,3])
  b = np.array([4,5,6])
  c = np.vstack((a,b))
  print("c:\n",c)
  
  # c:
  # [[1 2 3]
  #  [4 5 6]]
  ```

  

- `np.mgrid[]`

  è¿”å›np.arrayæ•°ç»„ï¼Œå¯åŒæ—¶è¿”å›å¤šç»„ï¼Œæ¯ä¸ªæ•°ç»„å®šä¹‰ [èµ·å§‹å€¼ ç»“æŸå€¼ æ­¥é•¿)

  `np.mgrid[ èµ·å§‹å€¼: ç»“æŸå€¼: æ­¥é•¿ï¼Œèµ·å§‹å€¼: ç»“æŸå€¼: æ­¥é•¿, â€¦ ]`

- `x.ravel()`

  å¤šç»´æ•°ç»„å˜ä¸€ç»´æ•°ç»„ï¼Œå°†xå˜ä¸ºä¸€ç»´æ•°ç»„ï¼Œâ€œæŠŠ **.** å‰å˜é‡æ‹‰ç›´â€

- `np.c_[ æ•°ç»„1ï¼Œæ•°ç»„2ï¼Œâ€¦ ]`

  è¿”å›çš„æ•°ç»„å„å…ƒç´ é…å¯¹

```python
import numpyas np
x, y = np.mgrid[1:3:1, 2:4:0.5]
grid = np.c_[x.ravel(), y.ravel()]
print("x:",x)
print("y:",y)
print('grid:\n', grid)

è¿è¡Œç»“æœï¼š
x = [[1. 1. 1. 1.]
	 [2. 2. 2. 2.]]
y = [[2. 2.5 3. 3.5]
	 [2. 2.5 3. 3.5]]

grid:
    [[1. 2. ]
     [1. 2.5]
     [1. 3. ]
     [1. 3.5]
     [2. 2. ]
     [2. 2.5]
     [2. 3. ]
     [2. 3.5]]
```











### æ•°å­¦è¿ç®—

![image-20210909153802163](img/image-20210909153802163.png)

- å¯¹åº”å…ƒç´ å››åˆ™è¿ç®—

  >åªæœ‰ç»´åº¦ç›¸åŒçš„å¼ é‡æ‰å¯ä»¥åšå››åˆ™è¿ç®—

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸åŠ 
    `tf.add(å¼ é‡1ï¼Œå¼ é‡2)`

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸å‡
    `tf.subtract(å¼ é‡1ï¼Œå¼ é‡2)`

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸ä¹˜
    `tf.multiply(å¼ é‡1ï¼Œå¼ é‡2)`

  - å®ç°ä¸¤ä¸ªå¼ é‡çš„å¯¹åº”å…ƒç´ ç›¸é™¤
    `tf.divide(å¼ é‡1ï¼Œå¼ é‡2)`



```python
a = tf.ones([1, 3])
b = tf.fill([1, 3], 3.)
print(a)
print(b)
print(tf.add(a,b))
print(tf.subtract(a,b))
print(tf.multiply(a,b))
print(tf.divide(b,a))

# tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32
# tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)
# tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)
```



- å¹³æ–¹ã€æ¬¡æ–¹ã€å¼€æ–¹
  - è®¡ç®—æŸä¸ªå¼ é‡çš„å¹³æ–¹
    `tf.square(å¼ é‡å)`
  - è®¡ç®—æŸä¸ªå¼ é‡çš„næ¬¡æ–¹
    `tf.pow(å¼ é‡åï¼Œnæ¬¡æ–¹æ•°)`
  - è®¡ç®—æŸä¸ªå¼ é‡çš„å¼€æ–¹
    `tf.sqrt(å¼ é‡åï¼‰`

```python
a = tf.fill([1, 2], 3.)
print(a)
print(tf.pow(a, 3))
print(tf.square(a))
print(tf.sqrt(a))

# tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)
# tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)
# tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)
# tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)
```



- çŸ©é˜µä¹˜
  - å®ç°ä¸¤ä¸ªçŸ©é˜µçš„ç›¸ä¹˜
    `tf.matmul(çŸ©é˜µ1ï¼ŒçŸ©é˜µ2)`

```python
a = tf.ones([3, 2])
b = tf.fill([2, 3], 3.)
print(tf.matmul(a, b))

# tf.Tensor([[6. 6. 6.]
#            [6. 6. 6.]
#            [6. 6. 6.]], shape=(3, 3), dtype=float32)
```



### æ–¹æ³•

#### `tf.Variable`

`tf.Variable()`å°†å˜é‡æ ‡è®°ä¸ºâ€œå¯è®­ç»ƒâ€ï¼Œè¢«æ ‡è®°çš„å˜é‡ä¼šåœ¨åå‘ä¼ æ’­ä¸­è®°å½•æ¢¯åº¦ä¿¡æ¯ã€‚ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ï¼Œå¸¸ç”¨è¯¥å‡½æ•°æ ‡è®°å¾…è®­ç»ƒå‚æ•°ã€‚

```python
tf.Variable(åˆå§‹å€¼)

w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))
```



#### `tf.data.Dataset.from_tensor_slices`

åˆ‡åˆ†ä¼ å…¥å¼ é‡çš„ç¬¬ä¸€ç»´åº¦ï¼Œç”Ÿæˆè¾“å…¥ç‰¹å¾/æ ‡ç­¾å¯¹ï¼Œæ„å»ºæ•°æ®é›†ï¼ˆNumpyå’ŒTensoræ ¼å¼éƒ½å¯ç”¨è¯¥è¯­å¥è¯»å…¥æ•°æ®ï¼‰

```python
data = tf.data.Dataset.from_tensor_slices((è¾“å…¥ç‰¹å¾, æ ‡ç­¾))

features = tf.random.normal([4, 3], mean=5, stddev=1)
labels = tf.constant([0, 1, 1, 0])
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
print(dataset)
for element in dataset:
	print(element)
    
# <TensorSliceDataset shapes: ((3,), ()), types: (tf.float32, tf.int32)>  ï¼ˆç‰¹å¾ï¼Œæ ‡ç­¾ï¼‰é…å¯¹
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5.0158296, 5.0271997, 6.399684 ],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=0>)
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5.150814 , 6.5250745, 5.037866 ],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=1>)
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([6.8031845, 5.809286 , 6.5262794],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=1>)
# (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4.4880443, 4.933515 , 5.1308255],dtype=float32)>, 	<tf.Tensor: shape=(), dtype=int32, numpy=0>)
```



#### `tf.GradientTape`

withç»“æ„è®°å½•è®¡ç®—è¿‡ç¨‹ï¼Œgradientæ±‚å‡ºå¼ é‡çš„æ¢¯åº¦

```python
with tf.GradientTape() as tape:
	è‹¥å¹²ä¸ªè®¡ç®—è¿‡ç¨‹
grad=tape.gradient(å‡½æ•°ï¼Œå¯¹è°æ±‚å¯¼)
```



```python
with tf.GradientTape() as tape:
	w = tf.Variable(tf.constant(3.0))
	loss = tf.pow(w, 2)
grad = tape.gradient(loss, w)
print(grad)

# tf.Tensor(6.0, shape=(), dtype=float32)
```



#### `enumerate`

enumerateæ˜¯pythonçš„å†…å»ºå‡½æ•°ï¼Œå®ƒå¯éå†æ¯ä¸ªå…ƒç´ (å¦‚åˆ—è¡¨ã€å…ƒç»„æˆ–å­—ç¬¦ä¸²)

ç»„åˆä¸ºï¼š<ç´¢å¼•ï¼Œå…ƒç´ >ï¼Œå¸¸åœ¨forå¾ªç¯ä¸­ä½¿ç”¨ã€‚
`enumerate(åˆ—è¡¨å)`

```python
seq= ['one', 'two', 'three']
for i, element in enumerate(seq):
	print(i, element)

# 0 one
# 1 two
# 2 three
```



#### `tf.one_hot`

ç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodingï¼‰ï¼šåœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¸¸ç”¨ç‹¬çƒ­ç åšæ ‡ç­¾ï¼Œæ ‡è®°ç±»åˆ«ï¼š1è¡¨ç¤ºæ˜¯ï¼Œ0è¡¨ç¤ºé

`tf.one_hot(å¾…è½¬æ¢æ•°æ®, depth=å‡ åˆ†ç±»)`

```python
classes = 3
labels = tf.constant([1,0,2]) # è¾“å…¥çš„å…ƒç´ å€¼æœ€å°ä¸º0ï¼Œæœ€å¤§ä¸º2
output = tf.one_hot(labels, depth=classes )
print(output)

# [[0. 1. 0.]
#  [1. 0. 0.]
#  [0. 0. 1.]], shape=(3, 3), dtype=float32)
```



#### `tf.nn.softmax`

![image-20210909161915094](img/image-20210909161915094.png)



```python
y = tf.constant( [1.01, 2.01, -0.66] )
y_pro= tf.nn.softmax(y)
print("After softmax, y_prois:", y_pro)

# After softmax, y_prois: tf.Tensor([0.255981740.695830460.0481878], shape=(3,), dtype=float32)
```



#### `assign_sub`

èµ‹å€¼æ“ä½œï¼Œæ›´æ–°å‚æ•°çš„å€¼å¹¶è¿”å›

è°ƒç”¨assign_subå‰ï¼Œå…ˆç”¨tf.Variableå®šä¹‰å˜é‡wä¸ºå¯è®­ç»ƒï¼ˆå¯è‡ªæ›´æ–°ï¼‰

`w.assign_sub(wè¦è‡ªå‡çš„å†…å®¹)`

```python
w = tf.Variable(4)
w.assign_sub(1)
print(w)  # w-=1
```



#### `tf.argmax`

è¿”å›å¼ é‡æ²¿æŒ‡å®šç»´åº¦æœ€å¤§å€¼çš„ç´¢å¼•
`tf.argmax(å¼ é‡å,axis=æ“ä½œè½´)`

```python
import numpyas np
test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])
print(test)
print(tf.argmax(test, axis=0)) # è¿”å›æ¯ä¸€åˆ—æœ€å¤§å€¼çš„ç´¢å¼•
print(tf.argmax(test, axis=1)) # è¿”å›æ¯ä¸€è¡Œæœ€å¤§å€¼çš„ç´¢å¼•

# [[1 2 3]
#  [2 3 4]
#  [5 4 3]
#  [8 7 2]]
# tf.Tensor([3 3 1], shape=(3,), dtype=int64)
# tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)
```



### æ•°æ®é›†è¯»å–

- é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆIrisï¼‰

  ```python
  from sklearn.datasetsimport load_iris
  x_data= datasets.load_iris().data   # è¿”å›irisæ•°æ®é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾
  y_data= datasets.load_iris().target # è¿”å›irisæ•°æ®é›†æ‰€æœ‰æ ‡ç­¾
  ```



## å­¦ä¹ ç‡

![image-20210910151223855](img/image-20210910151223855.png)



- æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
  å¯ä»¥å…ˆç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼Œå¿«é€Ÿå¾—åˆ°è¾ƒä¼˜è§£ï¼Œç„¶åé€æ­¥å‡å°å­¦ä¹ ç‡ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒåæœŸç¨³å®šã€‚

  `æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡= åˆå§‹å­¦ä¹ ç‡* å­¦ä¹ ç‡è¡°å‡ç‡ï¼ˆå½“å‰è½®æ•°/ å¤šå°‘è½®è¡°å‡ä¸€æ¬¡ï¼‰`



## æ¿€æ´»å‡½æ•°

![image-20210910151737249](img/image-20210910151737249.png)

![image-20210910220550382](img/image-20210910220550382.png)



- sigmoid

  ![image-20210910151840754](img/image-20210910151840754.png)

  ç‰¹ç‚¹
  ï¼ˆ1ï¼‰æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±

  â€‹		æ¢¯åº¦å¤§å° (0, 0.25] ï¼Œç´¯ä¹˜åé€ æˆæ¢¯åº¦æ¶ˆå¤±

  ï¼ˆ2ï¼‰è¾“å‡ºé0æ­£æ•°ï¼Œå‡å€¼é0ï¼Œæ”¶æ•›æ…¢

  ï¼ˆ3ï¼‰å¹‚è¿ç®—å¤æ‚ï¼Œè®­ç»ƒæ—¶é—´é•¿



- Tanh

  ![image-20210910151933750](img/image-20210910151933750.png)

  ç‰¹ç‚¹
  ï¼ˆ1ï¼‰è¾“å‡ºæ˜¯0å‡å€¼

  ï¼ˆ2ï¼‰æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±

  ï¼ˆ3ï¼‰å¹‚è¿ç®—å¤æ‚ï¼Œè®­ç»ƒæ—¶é—´é•¿



- Relu

  ![image-20210910152241269](img/image-20210910152241269.png)

  

  ä¼˜ç‚¹ï¼š

  ï¼ˆ1ï¼‰è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜(åœ¨æ­£åŒºé—´)

  ï¼ˆ2ï¼‰åªéœ€åˆ¤æ–­è¾“å…¥æ˜¯å¦å¤§äº0ï¼Œè®¡ç®—é€Ÿåº¦å¿«

  ï¼ˆ3ï¼‰æ”¶æ•›é€Ÿåº¦è¿œå¿«äºsigmoidå’Œtanh
  ç¼ºç‚¹ï¼š
  ï¼ˆ1ï¼‰è¾“å‡ºé0å‡å€¼ï¼Œæ”¶æ•›æ…¢

  ï¼ˆ2ï¼‰Dead RelUé—®é¢˜ï¼šæŸäº›ç¥ç»å…ƒå¯èƒ½æ°¸è¿œä¸ä¼šè¢«æ¿€æ´»ï¼Œå¯¼è‡´ç›¸åº”çš„å‚æ•°æ°¸è¿œä¸èƒ½è¢«æ›´æ–°ã€‚



- Leaky Relu

  ![image-20210910152543206](img/image-20210910152543206.png)



## æŸå¤±å‡½æ•°

![image-20210910221002716](img/image-20210910221002716.png)



### å‡æ–¹è¯¯å·®

![image-20210910221256343](img/image-20210910221256343.png)



### äº¤å‰ç†µ

![image-20210910221422551](img/image-20210910221422551.png)

```python
loss_ce1=tf.losses.categorical_crossentropy([1,0],[0.6,0.4])
loss_ce2=tf.losses.categorical_crossentropy([1,0],[0.8,0.2])
print("loss_ce1:", loss_ce1)
print("loss_ce2:", loss_ce2)

è¿è¡Œç»“æœï¼š
loss_ce1: tf.Tensor(0.5108256, shape=(), dtype=float32)
loss_ce2: tf.Tensor(0.2231435, shape=(), dtype=float32)
    
```



- `softmax`ä¸äº¤å‰ç†µç»“åˆ

  è¾“å‡ºå…ˆè¿‡`softmax`å‡½æ•°ï¼Œå†è®¡ç®—yä¸y_çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

  `tf.nn.softmax_cross_entropy_with_logits(y_ï¼Œy)`

  ```python
  y_ = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])
  y = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]])
  y_pro = tf.nn.softmax(y)
  loss_ce1 = tf.losses.categorical_crossentropy(y_, y_pro)
  loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)
  print('åˆ†æ­¥è®¡ç®—çš„ç»“æœ:\n', loss_ce1)
  print('ç»“åˆè®¡ç®—çš„ç»“æœ:\n', loss_ce2)
  
  åˆ†æ­¥è®¡ç®—çš„ç»“æœ:
  tf.Tensor([1.68795487e-041.03475622e-036.58839038e-022.58349207e+005.49852354e-02],shape=(5,),dtype=float64)
  ç»“åˆè®¡ç®—çš„ç»“æœ:
  tf.Tensor([1.68795487e-041.03475622e-036.58839038e-022.58349207e+005.49852354e-02],shape=(5,),dtype=float64)
  ```

  

### æ‰‹å†™æ¢¯åº¦ä¸‹é™è¿‡ç¨‹

```python
w = tf.Variable(tf.constant(5, dtype=tf.float32))  # åˆå§‹åŒ–æ—¶å€™èµ‹å€¼ä¸º5
lr = 0.01
epoch = 400

for epoch in range(epoch):  # for epoch å®šä¹‰é¡¶å±‚å¾ªç¯ï¼Œè¡¨ç¤ºå¯¹æ•°æ®é›†å¾ªç¯ epoch æ¬¡
    with tf.GradientTape() as tape:  # with ç»“æ„åˆ° grads æ¡†èµ·äº†æ¢¯åº¦çš„è®¡ç®—è¿‡ç¨‹
        loss = tf.square(w + 1)      # loss é€šè¿‡è°ƒæ•´ w çš„å€¼ä½¿ loss æœ€å°
    grads = tape.gradient(loss, w)   # .gradient å‡½æ•°å‘ŠçŸ¥è°å¯¹è°æ±‚å¯¼

    w.assign_sub(lr * grads)  # .assign_sub å¯¹å˜é‡åšè‡ªå‡ w -= lr*grads
    print("After %s epoch,w is %f,loss is %f" % (epoch, w.numpy(), loss))
```





## æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ

![image-20210910222051194](img/image-20210910222051194.png)



### æ­£åˆ™é¡¹

![image-20210910222200719](img/image-20210910222200719.png)



- L1å’ŒL2

  ![image-20210910222236869](img/image-20210910222236869.png)

  

```python
with tf.GradientTape() as tape:  # è®°å½•æ¢¯åº¦ä¿¡æ¯
    h1 = tf.matmul(x_train, w1) + b1  # è®°å½•ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
    h1 = tf.nn.relu(h1)
    y = tf.matmul(h1, w2) + b2

    # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
    loss_mse = tf.reduce_mean(tf.square(y_train - y))

    # æ·»åŠ l2æ­£åˆ™åŒ–
    loss_regularization = []
    # å†…éƒ¨ç»†èŠ‚ tf.nn.l2_loss(w) = sum(w ** 2) / 2
    loss_regularization.append(tf.nn.l2_loss(w1))
    loss_regularization.append(tf.nn.l2_loss(w2))
    # æ±‚å’Œï¼Œæ±‚è§£æ­£åˆ™é¡¹
    loss_regularization = tf.reduce_sum(loss_regularization)

    loss = loss_mse + 0.03 * loss_regularization  # REGULARIZER = 0.03

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
variables = [w1, b1, w2, b2]
grads = tape.gradient(loss, variables)

# å®ç°æ¢¯åº¦æ›´æ–°
# w1 = w1 - lr * w1_grad
w1.assign_sub(lr * grads[0])
b1.assign_sub(lr * grads[1])
w2.assign_sub(lr * grads[2])
b2.assign_sub(lr * grads[3])
```



## ä¼˜åŒ–å™¨

![image-20210910223516582](img/image-20210910223516582.png)



### SGD

![image-20210910223604954](img/image-20210910223604954.png)



### SGDM

![image-20210910223721645](img/image-20210910223721645.png)



```python
m_w, m_b = 0, 0
beta = 0.9

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# sgd-momentun  
m_w = beta * m_w + (1 - beta) * grads[0]
m_b = beta * m_b + (1 - beta) * grads[1]
w1.assign_sub(lr * m_w)
b1.assign_sub(lr * m_b)
```



### Adagrad

![image-20210910224027009](img/image-20210910224027009.png)



```python
# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# adagrad
v_w += tf.square(grads[0])
v_b += tf.square(grads[1])
w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))
```



### RMSProp

![image-20210910224242736](img/image-20210910224242736.png)



```python
v_w, v_b = 0, 0
beta = 0.9

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# rmsprop
v_w = beta * v_w + (1 - beta) * tf.square(grads[0])
v_b = beta * v_b + (1 - beta) * tf.square(grads[1])
w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))
```



### Adam

![image-20210910224559530](img/image-20210910224559530.png)



```python
m_w, m_b = 0, 0
v_w, v_b = 0, 0
beta1, beta2 = 0.9, 0.999
delta_w, delta_b = 0, 0
global_step = 0

# è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
grads = tape.gradient(loss, [w1, b1])

# adam
m_w = beta1 * m_w + (1 - beta1) * grads[0]
m_b = beta1 * m_b + (1 - beta1) * grads[1]
v_w = beta2 * v_w + (1 - beta2) * tf.square(grads[0])
v_b = beta2 * v_b + (1 - beta2) * tf.square(grads[1])

m_w_correction = m_w / (1 - tf.pow(beta1, int(global_step)))
m_b_correction = m_b / (1 - tf.pow(beta1, int(global_step)))
v_w_correction = v_w / (1 - tf.pow(beta2, int(global_step)))
v_b_correction = v_b / (1 - tf.pow(beta2, int(global_step)))

w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))
b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))
```



## æ­å»ºç½‘ç»œå…«è‚¡

**å…­æ­¥æ³•ï¼š**

1. **import**
2. **train, test**
3. **model = tf.keras.models.Sequential**
4. **model.compile**
5. **model.fit**
6. **model.summary**



### Basesline

- IRIS-Sequential

  ```python
  import tensorflow as tf
  from sklearn import datasets
  import numpy as np
  
  x_train = datasets.load_iris().data
  y_train = datasets.load_iris().target
  
  np.random.seed(116)
  np.random.shuffle(x_train)
  np.random.seed(116)
  np.random.shuffle(y_train)
  tf.random.set_seed(116)
  
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())
  ])
  
  model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy'])
  
  model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)
  
  model.summary()
  ```

  

- MNIST-Sequential

  ![image-20210913185128467](img/image-20210913185128467.png)

  ```python
  import tensorflow as tf
  
  mnist = tf.keras.datasets.mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(),  # æ‹‰ç›´
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy']  # y_ï¼ˆçœŸå®ï¼‰æ˜¯æ•°å€¼ï¼Œyï¼ˆé¢„æµ‹ï¼‰æ˜¯ç‹¬çƒ­ç (æ¦‚ç‡åˆ†å¸ƒ)
               )
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  
  model.summary()
  ```




- FASHION-Sequential

  ```python
  import tensorflow as tf
  
  fashion = tf.keras.datasets.fashion_mnist
  (x_train, y_train),(x_test, y_test) = fashion.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy'])
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  model.summary()
  ```

  





### ç¬¬ä¸€æ­¥ï¼š`import`



### ç¬¬äºŒæ­¥ï¼š`trainã€test`

- è‡ªåˆ¶æ•°æ®é›†

  - å›¾ç‰‡è·¯å¾„ä¸‹å­˜å‚¨å›¾ç‰‡

  ![image-20210913214015589](img/image-20210913214015589.png)

  - æ ‡ç­¾æ–‡ä»¶ï¼ˆå›¾ç‰‡åç§°ï¼Œæ ‡ç­¾ï¼‰

    ![image-20210913214141104](img/image-20210913214141104.png)

  

  ```python
  # def generateds(å›¾ç‰‡è·¯å¾„, æ ‡ç­¾æ–‡ä»¶): 
  # åŠ è½½å›¾ç‰‡ è½¬æ¢æˆçŸ©é˜µ
  
  def generateds(path, txt):
      f = open(txt, 'r')
      contents = f.readlines()  # æŒ‰è¡Œè¯»å–
      f.close()
      x, y_ = [], []
      for content in contents:
          value = content.split()  # ä»¥ç©ºæ ¼åˆ†å¼€ï¼Œå­˜å…¥æ•°ç»„
          img_path = path + value[0]
          img = Image.open(img_path)
          img = np.array(img.convert('L'))
          img = img / 255.
          x.append(img)
          y_.append(value[1])
      print('loading : ' + "data")
  
      x = np.array(x)
      y_ = np.array(y_)
      y_ = y_.astype(np.int64)
      return x, y_
  ```

  

- æ•°æ®å¢å¼º

  ```python
  image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator(
                      rescale=æ‰€æœ‰æ•°æ®å°†ä¹˜ä»¥è¯¥æ•°å€¼
                      rotation_range=éšæœºæ—‹è½¬è§’åº¦æ•°èŒƒå›´
                      width_shift_range=éšæœºå®½åº¦åç§»é‡
                      height_shift_range=éšæœºé«˜åº¦åç§»é‡
                      æ°´å¹³ç¿»è½¬ï¼šhorizontal_flip=æ˜¯å¦éšæœºæ°´å¹³ç¿»è½¬
                      éšæœºç¼©æ”¾ï¼šzoom_range= éšæœºç¼©æ”¾çš„èŒƒå›´[1-nï¼Œ1+n])
  
  # ä¾‹ï¼šæ•°æ®å¢å¼ºï¼ˆå¢å¤§æ•°æ®é‡ï¼‰
  image_gen_train = ImageDataGenerator(
                      rescale=1. / 1.,   # å¦‚ä¸ºå›¾åƒï¼Œåˆ†æ¯ä¸º255æ—¶ï¼Œå¯å½’è‡³0ï½1
                      rotation_range=45, # éšæœº45åº¦æ—‹è½¬
      				width_shift_range=.15,  # å®½åº¦åç§»
                      height_shift_range=.15, # é«˜åº¦åç§»
      				horizontal_flip=False,  # æ°´å¹³ç¿»è½¬
                      zoom_range=0.5 	   # å°†å›¾åƒéšæœºç¼©æ”¾é˜ˆé‡50ï¼…
  					)
  
  
  # ç¬¬ä¸€æ­¥
  # (60000, 28, 28) --> (60000, 28, 28, 1)
  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
  
  # ç¬¬äºŒæ­¥
  image_gen_train.fit(x_train)
  
  # ç¬¬ä¸‰æ­¥
  # Baseline: model.fit(x_train, y_train, batch_size=32, â€¦â€¦)
  model.fit(image_gen_train.flow(x_train, y_train,batch_size=32), â€¦â€¦)
  ```

  

### ç¬¬ä¸‰æ­¥ï¼š`Sequential`

`model = tf.keras.models.Sequential([ç½‘ç»œç»“æ„])`  # æè¿°å„å±‚ç½‘ç»œ

- ç½‘ç»œç»“æ„ä¸¾ä¾‹ï¼š
  - æ‹‰ç›´å±‚ï¼š`tf.keras.layers.Flatten()`

  - å…¨è¿æ¥å±‚ï¼š`tf.keras.layers.Dense(ç¥ç»å…ƒä¸ªæ•°, activation="æ¿€æ´»å‡½æ•°", kernel_regularizer=å“ªç§æ­£åˆ™åŒ–)`

    `activation`ï¼ˆå­—ç¬¦ä¸²ç»™å‡ºï¼‰å¯é€‰: reluã€softmaxã€sigmoid ã€tanh

    `kernel_regularizer`å¯é€‰:`tf.keras.regularizers.l1()`ã€`tf.keras.regularizers.l2()`



### ç¬¬å››æ­¥ï¼š`model.compile`

`model.compile(optimizer = ä¼˜åŒ–å™¨, loss = æŸå¤±å‡½æ•°, metrics = [â€œå‡†ç¡®ç‡â€] )`

- `optimizer`å¯é€‰:
  - â€˜sgdâ€™ or `tf.keras.optimizers.SGD(lr=å­¦ä¹ ç‡,momentum=åŠ¨é‡å‚æ•°)`
  - â€˜adagradâ€™ or `tf.keras.optimizers.Adagrad(lr=å­¦ä¹ ç‡)`
  - â€˜adadeltaâ€™ or `tf.keras.optimizers.Adadelta(lr=å­¦ä¹ ç‡)`
  - â€˜adamâ€™or `tf.keras.optimizers.Adam(lr=å­¦ä¹ ç‡, beta_1=0.9, beta_2=0.999)`
- `loss`å¯é€‰:
  - â€˜mseâ€™or `tf.keras.losses.MeanSquaredError()`
  - â€˜sparse_categorical_crossentropyâ€™ or `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)`
    - `from_logits=False` ç½‘ç»œæœ€åè¿›è¡Œäº†softmaxæ¦‚ç‡å½’ä¸€åŒ–
    - `from_logits=True`
- `metrics`å¯é€‰:
  - â€˜accuracyâ€™ ï¼šy\_å’Œyéƒ½æ˜¯æ•°å€¼ï¼Œå¦‚ y\_=[1]ï¼Œy=[1]
  - â€˜categorical_accuracyâ€™ ï¼šy\_å’Œyéƒ½æ˜¯ç‹¬çƒ­ç (æ¦‚ç‡åˆ†å¸ƒ)ï¼Œå¦‚ y_=[0,1,0]ï¼Œy=[0.256,0.695,0.048]
  - â€˜sparse_categorical_accuracyâ€™ ï¼šy\_æ˜¯æ•°å€¼ï¼Œyæ˜¯ç‹¬çƒ­ç (æ¦‚ç‡åˆ†å¸ƒ)ï¼Œå¦‚ y\_=[1]ï¼Œy=[0.256,0.695,0.048]



### ç¬¬äº”æ­¥ï¼š`model.fit`

```python
model.fit(è®­ç»ƒé›†çš„è¾“å…¥ç‰¹å¾, è®­ç»ƒé›†çš„æ ‡ç­¾, 
          batch_size= , epochs= ,
          validation_data=(æµ‹è¯•é›†çš„è¾“å…¥ç‰¹å¾ï¼Œæµ‹è¯•é›†çš„æ ‡ç­¾),
          validation_split=ä»è®­ç»ƒé›†åˆ’åˆ†å¤šå°‘æ¯”ä¾‹ç»™æµ‹è¯•é›†,
          validation_freq= å¤šå°‘æ¬¡epochæµ‹è¯•ä¸€æ¬¡)
```



### ç¬¬å…­æ­¥ï¼š`model.summary`

![image-20210913183347002](img/image-20210913183347002.png)







## ç»§æ‰¿`Model`

- åŸºç¡€ç»“æ„

```python
classMyModel(Model):
	def __init__(self):
		super(MyModel, self).__init__()
		å®šä¹‰ç½‘ç»œç»“æ„å—
	def call(self, x):
		è°ƒç”¨ç½‘ç»œç»“æ„å—ï¼Œå®ç°å‰å‘ä¼ æ’­
		return y
    
model = MyModel() # æ¨¡å‹å®ä¾‹åŒ–

# ==============================================
class IrisModel(Model):
    def __init__(self):
        super(IrisModel, self).__init__()
        self.d1 = Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())

    def call(self, x):
        y = self.d1(x)
        return y

model = IrisModel()
```



- MNIST-Model

  ```python
  import tensorflow as tf
  from tensorflow.keras.layers import Dense, Flatten
  from tensorflow.keras import Model
  
  mnist = tf.keras.datasets.mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  
  class MnistModel(Model):
      def __init__(self):
          super(MnistModel, self).__init__()
          self.flatten = Flatten()
          self.d1 = Dense(128, activation='relu')
          self.d2 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.flatten(x)
          x = self.d1(x)
          y = self.d2(x)
          return y
  
  
  model = MnistModel()
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy']
               )
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  
  model.summary()
  ```

  

- FASHION-Model

  ```python
  import tensorflow as tf
  from tensorflow.keras.layers import Dense, Flatten
  from tensorflow.keras import Model
  
  fashion = tf.keras.datasets.fashion_mnist
  (x_train, y_train),(x_test, y_test) = fashion.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  
  class MnistModel(Model):
      def __init__(self):
          super(MnistModel, self).__init__()
          self.flatten = Flatten()
          self.d1 = Dense(128, activation='relu')
          self.d2 = Dense(10, activation='softmax')
  
      def call(self, x):
          x = self.flatten(x)
          x = self.d1(x)
          y = self.d2(x)
          return y
  
  
  model = MnistModel()
  
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                metrics=['sparse_categorical_accuracy'])
  
  model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
  model.summary()
  
  ```



## è¯»å–ä¿å­˜æ¨¡å‹

### è¯»å–æ¨¡å‹

```python
# è¯»å–æ¨¡å‹
checkpoint_save_path = "./checkpoint/fashion.ckpt"
if os.path.exists(checkpoint_save_path + '.index'):
    print('-------------load the model-----------------')
    model.load_weights(checkpoint_save_path)
```



### ä¿å­˜æ¨¡å‹

```python
tf.keras.callbacks.ModelCheckpoint(
                    filepath=è·¯å¾„æ–‡ä»¶å, 
                    save_weights_only=True/False, 
                    save_best_only=True/False)

history = model.fit(callbacks=[cp_callback])


# ä¾‹
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                 save_weights_only=True,
                                                 save_best_only=True)

history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback])
```



### å‚æ•°æå–

- è¿”å›æ¨¡å‹ä¸­å¯è®­ç»ƒçš„å‚æ•°:

  `model.trainable_variables`

- è®¾ç½®printè¾“å‡ºæ ¼å¼

  `np.set_printoptions(threshold=è¶…è¿‡å¤šå°‘çœç•¥æ˜¾ç¤º)`

```python
np.set_printoptions(threshold=np.inf)# np.infè¡¨ç¤ºæ— é™å¤§
print(model.trainable_variables)

file = open('./weights.txt', 'w')
for v in model.trainable_variables:
    file.write(str(v.name) + '\n')
    file.write(str(v.shape) + '\n')
    file.write(str(v.numpy()) + '\n')
file.close()
```



## ç»˜åˆ¶`acc/loss`æ›²çº¿

```python
history=model.fit(è®­ç»ƒé›†æ•°æ®, 
                  è®­ç»ƒé›†æ ‡ç­¾, 
                  batch_size=, 
                  epochs=,
                  validation_split=ç”¨ä½œæµ‹è¯•æ•°æ®çš„æ¯”ä¾‹,
                  validation_data=æµ‹è¯•é›†,
                  validation_freq=æµ‹è¯•é¢‘ç‡)
```



- historyï¼š

  - è®­ç»ƒé›†lossï¼š`loss`
  - æµ‹è¯•é›†lossï¼š`val_loss`
  - è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š`sparse_categorical_accuracy`
  - æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š`val_sparse_categorical_accuracy`

  ```python
  acc = history.history['sparse_categorical_accuracy']
  val_acc = history.history['val_sparse_categorical_accuracy']
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  ```



- ç»˜å›¾

```python
from matplotlib import pyplot as plt

# æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„accå’Œlossæ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.show()
```



## æ¨¡å‹é¢„æµ‹

- è¿”å›å‰å‘ä¼ æ’­è®¡ç®—ç»“æœ

  `predict(è¾“å…¥ç‰¹å¾, batch_size=æ•´æ•°)`

```python
# æ•°æ®é¢„å¤„ç†
result = model.predict(x_predict)
pred = tf.argmax(result, axis=1) # å–æ¦‚ç‡æœ€å¤§å€¼
```



# æ‰‹å†™ç¥ç»ç½‘ç»œIrisåˆ†ç±»

```python
# å¯¼å…¥æ‰€éœ€æ¨¡å—
import tensorflow as tf
from sklearn import datasets
from matplotlib import pyplot as plt
import numpy as np

# ====================================================================

# å¯¼å…¥æ•°æ®ï¼Œåˆ†åˆ«ä¸ºè¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target
print(x_data.shape, y_data.shape)

# éšæœºæ‰“ä¹±æ•°æ®ï¼ˆå› ä¸ºåŸå§‹æ•°æ®æ˜¯é¡ºåºçš„ï¼Œé¡ºåºä¸æ‰“ä¹±ä¼šå½±å“å‡†ç¡®ç‡ï¼‰
# seed: éšæœºæ•°ç§å­ï¼Œæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå½“è®¾ç½®ä¹‹åï¼Œæ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°éƒ½ä¸€æ ·
np.random.seed(116)  # ä½¿ç”¨ç›¸åŒçš„seedï¼Œä¿è¯è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾ä¸€ä¸€å¯¹åº”
np.random.shuffle(x_data)
np.random.seed(116)
np.random.shuffle(y_data)
tf.random.set_seed(116)

# å°†æ‰“ä¹±åçš„æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†ä¸ºå‰120è¡Œï¼Œæµ‹è¯•é›†ä¸ºå30è¡Œ
x_train = x_data[:-30]
y_train = y_data[:-30]
x_test = x_data[-30:]
y_test = y_data[-30:]

# è½¬æ¢xçš„æ•°æ®ç±»å‹ï¼Œå¦åˆ™åé¢çŸ©é˜µç›¸ä¹˜æ—¶ä¼šå› æ•°æ®ç±»å‹ä¸ä¸€è‡´æŠ¥é”™
x_train = tf.cast(x_train, tf.float32)
x_test = tf.cast(x_test, tf.float32)

# from_tensor_sliceså‡½æ•°ä½¿è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾å€¼ä¸€ä¸€å¯¹åº”ã€‚ï¼ˆæŠŠæ•°æ®é›†åˆ†æ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡batchç»„æ•°æ®ï¼‰
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

# ====================================================================

# ç”Ÿæˆç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œ4ä¸ªè¾“å…¥ç‰¹å¾æ•…ï¼Œè¾“å…¥å±‚ä¸º4ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼›å› ä¸º3åˆ†ç±»ï¼Œæ•…è¾“å‡ºå±‚ä¸º3ä¸ªç¥ç»å…ƒ
# ç”¨tf.Variable()æ ‡è®°å‚æ•°å¯è®­ç»ƒ
# ä½¿ç”¨seedä½¿æ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°ç›¸åŒ
w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))
b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))

lr = 0.1  # å­¦ä¹ ç‡ä¸º0.1
train_loss_results = []  # å°†æ¯æ¬¡è¿­ä»£çš„lossè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»lossæ›²çº¿æä¾›æ•°æ®
test_acc = []  # å°†æ¯æ¬¡è¿­ä»£çš„accè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»accæ›²çº¿æä¾›æ•°æ®
epoch = 500  # è¿­ä»£500æ¬¡
loss_all = 0  # æ¯æ¬¡è¿­ä»£åˆ†å¤šä¸ªbatchï¼Œloss_allè®°å½•æ‰€æœ‰batchç”Ÿæˆçš„losså’Œ

# ====================================================================

# è®­ç»ƒéƒ¨åˆ†
for epoch in range(epoch):  # æ•°æ®é›†çº§åˆ«çš„å¾ªç¯ï¼Œæ¯ä¸ªepochå¾ªç¯ä¸€æ¬¡å®Œæ•´æ•°æ®é›†
    for step, (x_train, y_train) in enumerate(train_db):  # batchçº§åˆ«çš„å¾ªç¯ ï¼Œæ¯ä¸ªstepå¾ªç¯ä¸€ä¸ªbatch
        with tf.GradientTape() as tape:  # withç»“æ„è®°å½•æ¢¯åº¦ä¿¡æ¯
            y = tf.matmul(x_train, w1) + b1  # ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
            y = tf.nn.softmax(y)  # ä½¿è¾“å‡ºyç¬¦åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ­¤æ“ä½œåä¸ç‹¬çƒ­ç åŒé‡çº§ï¼Œå¯ç›¸å‡æ±‚lossï¼‰
            y_ = tf.one_hot(y_train, depth=3)  # å°†æ ‡ç­¾å€¼è½¬æ¢ä¸ºç‹¬çƒ­ç æ ¼å¼ï¼Œæ–¹ä¾¿è®¡ç®—losså’Œaccuracy
            loss = tf.reduce_mean(tf.square(y_ - y))  # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
            loss_all += loss.numpy()  # å°†æ¯ä¸ªstepè®¡ç®—å‡ºçš„lossç´¯åŠ ï¼Œä¸ºåç»­æ±‚losså¹³å‡å€¼æä¾›æ•°æ®ï¼Œè¿™æ ·è®¡ç®—çš„lossæ›´å‡†ç¡®
        # è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
        grads = tape.gradient(loss, [w1, b1])

        # å®ç°æ¢¯åº¦æ›´æ–°
        w1.assign_sub(lr * grads[0])  # å‚æ•°w1è‡ªæ›´æ–° w1 = w1 - lr * w1_grad
        b1.assign_sub(lr * grads[1])  # å‚æ•°bè‡ªæ›´æ–°  b = b - lr * b_grad

    # æ¯ä¸ªepochï¼Œæ‰“å°lossä¿¡æ¯
    if epoch % 50 == 0:
        print("Epoch {}, loss: {}".format(epoch, loss_all / 4))
    train_loss_results.append(loss_all / 4)  # å°†4ä¸ªstepçš„lossæ±‚å¹³å‡è®°å½•åœ¨æ­¤å˜é‡ä¸­
    loss_all = 0  # loss_allå½’é›¶ï¼Œä¸ºè®°å½•ä¸‹ä¸€ä¸ªepochçš„lossåšå‡†å¤‡

    # æµ‹è¯•éƒ¨åˆ†
    # total_correctä¸ºé¢„æµ‹å¯¹çš„æ ·æœ¬ä¸ªæ•°, total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œå°†è¿™ä¸¤ä¸ªå˜é‡éƒ½åˆå§‹åŒ–ä¸º0
    total_correct, total_number = 0, 0
    for x_test, y_test in test_db:
        # ä½¿ç”¨æ›´æ–°åçš„å‚æ•°è¿›è¡Œé¢„æµ‹
        y = tf.matmul(x_test, w1) + b1
        y = tf.nn.softmax(y)
        # è¿”å›yä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„åˆ†ç±»
        pred = tf.argmax(y, axis=1)
        # å°†predè½¬æ¢ä¸ºy_testçš„æ•°æ®ç±»å‹
        pred = tf.cast(pred, dtype=y_test.dtype)
        
        # è‹¥åˆ†ç±»æ­£ç¡®ï¼Œåˆ™correct=1ï¼Œå¦åˆ™ä¸º0ï¼Œå°†boolå‹çš„ç»“æœè½¬æ¢ä¸ºintå‹
        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)
        # å°†æ¯ä¸ªbatchçš„correctæ•°åŠ èµ·æ¥
        correct = tf.reduce_sum(correct)
        
        # å°†æ‰€æœ‰batchä¸­çš„correctæ•°åŠ èµ·æ¥
        total_correct += int(correct)
        # total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œä¹Ÿå°±æ˜¯x_testçš„è¡Œæ•°ï¼Œshape[0]è¿”å›å˜é‡çš„è¡Œæ•°
        total_number += x_test.shape[0]
        
    # æ€»çš„å‡†ç¡®ç‡ç­‰äºtotal_correct/total_number
    acc = total_correct / total_number
    test_acc.append(acc)
    if epoch % 50 == 0:
        print("Test_acc:", acc)
        print("--------------------------")
        
# ====================================================================  

# ç»˜åˆ¶ loss æ›²çº¿
plt.title('Loss Function Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')  # xè½´å˜é‡åç§°
plt.ylabel('Loss')   # yè½´å˜é‡åç§°
plt.plot(train_loss_results, label="$Loss$")  # é€ç‚¹ç”»å‡ºtrian_loss_resultså€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Loss
plt.legend()  # ç”»å‡ºæ›²çº¿å›¾æ ‡
plt.show()    # ç”»å‡ºå›¾åƒ

# ç»˜åˆ¶ Accuracy æ›²çº¿
plt.title('Acc Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')     # xè½´å˜é‡åç§°
plt.ylabel('Acc')       # yè½´å˜é‡åç§°
plt.plot(test_acc, label="$Accuracy$")  # é€ç‚¹ç”»å‡ºtest_accå€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Accuracy
plt.legend()
plt.show()
```



# å·ç§¯ç¥ç»ç½‘ç»œ

> å·ç§¯ç¥ç»ç½‘ç»œï¼šå€ŸåŠ©å·ç§¯æ ¸æå–ç‰¹å¾åï¼Œé€å…¥å…¨è¿æ¥ç½‘ç»œ

![image-20210914142029510](img/image-20210914142029510.png)



![image-20210914142135724](img/image-20210914142135724.png)



## å·ç§¯è®¡ç®—

> å·ç§¯è®¡ç®—å¯è®¤ä¸ºæ˜¯ä¸€ç§æœ‰æ•ˆæå–å›¾åƒç‰¹å¾çš„æ–¹æ³•

- åŸºæœ¬æµç¨‹

  å…ˆå¯¹åŸå§‹å›¾åƒè¿›è¡Œç‰¹å¾æå–å†æŠŠæå–åˆ°çš„ç‰¹å¾é€ç»™å…¨è¿æ¥ç½‘ç»œ

  ![image-20210914133938412](img/image-20210914133938412.png)

  - ä¸Šå›¾RBGè¾“å…¥ç‰¹å¾æ˜¯ä¸‰é€šé“
  - è¾“å…¥ç‰¹å¾å›¾çš„æ·±åº¦ï¼ˆchannelæ•°ï¼‰ï¼Œå†³å®šäº†å½“å‰å±‚å·ç§¯æ ¸çš„æ·±åº¦
  - å½“å‰å±‚å·ç§¯æ ¸çš„ä¸ªæ•°ï¼Œå†³å®šäº†å½“å‰å±‚è¾“å‡ºç‰¹å¾å›¾çš„æ·±åº¦



- å·ç§¯æ ¸

  ![image-20210914134449907](img/image-20210914134449907.png)





- å·ç§¯è®¡ç®—

  - æ·±åº¦ä¸º 1 çš„å·ç§¯æ ¸

  ![image-20210914134538610](img/image-20210914134538610.png)

  `(-1)*1+0*0+1*2+(-1)*5+0*4+1*2+(-1)*3+0*4+1*5+1=1`

  

  - æ·±åº¦ä¸º 3 çš„å·ç§¯æ ¸

<img src="img/image-20210914134646997.png" alt="image-20210914134646997" style="zoom: 67%;" />



## æ„Ÿå—é‡

æ„Ÿå—é‡ï¼ˆReceptive Fieldï¼‰ï¼šå·ç§¯ç¥ç»ç½‘ç»œå„è¾“å‡ºç‰¹å¾å›¾ä¸­çš„æ¯ä¸ªåƒç´ ç‚¹ï¼Œåœ¨åŸå§‹è¾“å…¥å›¾ç‰‡ä¸Šæ˜ å°„åŒºåŸŸçš„å¤§å°ã€‚

![image-20210914135238788](img/image-20210914135238788.png)



ç‰¹å¾å›¾è¾ƒå¤§æ—¶ï¼Œä¼˜å…ˆé‡‡ç”¨ä¸¤å±‚ 3 * 3 å·ç§¯æ ¸



## å…¨é›¶å¡«å……ï¼ˆPaddingï¼‰

è¾¹ç¼˜è¡¥0

![image-20210914135548578](img/image-20210914135548578.png)



- TensorFlowå…³é”®å­—

![image-20210914135513465](img/image-20210914135513465.png)



![image-20210914135609163](img/image-20210914135609163.png)



## å·ç§¯å±‚

```python
tf.keras.layers.Conv2D (
    filters = å·ç§¯æ ¸ä¸ªæ•°, 
    kernel_size = å·ç§¯æ ¸å°ºå¯¸,  # æ­£æ–¹å½¢å†™æ ¸é•¿æ•´æ•°ï¼Œæˆ–ï¼ˆæ ¸é«˜hï¼Œæ ¸å®½wï¼‰
    strides = æ»‘åŠ¨æ­¥é•¿,  # æ¨ªçºµå‘ç›¸åŒå†™æ­¥é•¿æ•´æ•°ï¼Œæˆ–(çºµå‘æ­¥é•¿hï¼Œæ¨ªå‘æ­¥é•¿w)ï¼Œé»˜è®¤1
    padding = â€œsameâ€ or â€œvalidâ€,  # ä½¿ç”¨å…¨é›¶å¡«å……æ˜¯â€œsameâ€ï¼Œä¸ä½¿ç”¨æ˜¯â€œvalidâ€ï¼ˆé»˜è®¤ï¼‰
    activation = â€œ reluâ€ or â€œ sigmoid â€ or â€œ tanh â€ or â€œ softmaxâ€ç­‰,  # å¦‚æœ‰BNæ­¤å¤„ä¸å†™
    input_shape = (é«˜, å®½, é€šé“æ•°)  # è¾“å…¥ç‰¹å¾å›¾ç»´åº¦ï¼Œå¯çœç•¥
)

# ä¾‹:
model = tf.keras.models.Sequential([
    Conv2D(6, 5, padding='valid', activation='sigmoid'),
    MaxPool2D(2, 2),
    Conv2D(6, (5, 5), padding='valid', activation='sigmoid'),
    MaxPool2D(2, (2, 2)),
	Conv2D(filters=6, kernel_size=(5, 5),padding='valid', activation='sigmoid'),
    MaxPool2D(pool_size=(2, 2), strides=2),
    Flatten(),
    Dense(10, activation='softmax')
])
```



## æ‰¹æ ‡å‡†åŒ–

> Batch Normalization (BN)

- æ ‡å‡†åŒ–ï¼šä½¿æ•°æ®ç¬¦åˆ0å‡å€¼ï¼Œ1ä¸ºæ ‡å‡†å·®çš„åˆ†å¸ƒã€‚
- æ‰¹æ ‡å‡†åŒ–ï¼šå¯¹ä¸€å°æ‰¹æ•°æ®ï¼ˆbatchï¼‰ï¼Œåšæ ‡å‡†åŒ–å¤„ç†ã€‚



- å‡å€¼æ ‡å‡†å·®ï¼šæ±‚è§£ä¸€ä¸ªbatchå†…ï¼Œç¬¬kä¸ªå·ç§¯æ ¸ï¼Œè¾“å‡ºç‰¹å¾å›¾ä¸­ï¼Œæ±‚æ‰€æœ‰å…ƒç´ çš„å‡å€¼ï¼Œæ ‡å‡†å·®

![image-20210914140348954](img/image-20210914140348954.png)

![image-20210914140402809](img/image-20210914140402809.png)



- å½’ä¸€åŒ–

![image-20210914140511017](img/image-20210914140511017.png)



- ä½œç”¨

  ![image-20210914140921204](img/image-20210914140921204.png)



### ç¼©æ”¾å› å­å’Œåç§»å› å­

>  ä¸ºæ¯ä¸ªå·ç§¯æ ¸å¼•å…¥å¯è®­ç»ƒå‚æ•°ğœ¸å’Œğœ·ï¼Œè°ƒæ•´æ‰¹å½’ä¸€åŒ–çš„åŠ›åº¦

![image-20210914140938416](img/image-20210914140938416.png)



### ä»£ç å®ç°

![image-20210914141154996](img/image-20210914141154996.png)



`tf.keras.layers.BatchNormalization()`

```python
model = tf.keras.models.Sequential([
    Conv2D(filters=6, kernel_size=(5, 5), padding='same'), # å·ç§¯å±‚
    BatchNormalization(), # BNå±‚
    Activation('relu'), # æ¿€æ´»å±‚
    MaxPool2D(pool_size=(2, 2), strides=2, padding='same'), # æ± åŒ–å±‚
    Dropout(0.2), # dropoutå±‚
])
```



## æ± åŒ–

> æ± åŒ–ç”¨äºå‡å°‘ç‰¹å¾æ•°æ®é‡

- æœ€å¤§å€¼æ± åŒ–å¯æå–å›¾ç‰‡çº¹ç†

- å‡å€¼æ± åŒ–å¯ä¿ç•™èƒŒæ™¯ç‰¹å¾

![image-20210914141405881](img/image-20210914141405881.png)



```python
# æœ€å¤§æ± åŒ–
tf.keras.layers.MaxPool2D(
    pool_size=æ± åŒ–æ ¸å°ºå¯¸ï¼Œ#æ­£æ–¹å½¢å†™æ ¸é•¿æ•´æ•°ï¼Œæˆ–ï¼ˆæ ¸é«˜hï¼Œæ ¸å®½wï¼‰
    strides=æ± åŒ–æ­¥é•¿ï¼Œ#æ­¥é•¿æ•´æ•°ï¼Œæˆ–(çºµå‘æ­¥é•¿hï¼Œæ¨ªå‘æ­¥é•¿w)ï¼Œé»˜è®¤ä¸ºpool_size
    padding=â€˜validâ€™orâ€˜sameâ€™#ä½¿ç”¨å…¨é›¶å¡«å……æ˜¯â€œsameâ€ï¼Œä¸ä½¿ç”¨æ˜¯â€œvalidâ€ï¼ˆé»˜è®¤ï¼‰
)

# å‡å€¼æ± åŒ–
tf.keras.layers.AveragePooling2D(
    pool_size=æ± åŒ–æ ¸å°ºå¯¸ï¼Œ#æ­£æ–¹å½¢å†™æ ¸é•¿æ•´æ•°ï¼Œæˆ–ï¼ˆæ ¸é«˜hï¼Œæ ¸å®½wï¼‰
    strides=æ± åŒ–æ­¥é•¿ï¼Œ#æ­¥é•¿æ•´æ•°ï¼Œæˆ–(çºµå‘æ­¥é•¿hï¼Œæ¨ªå‘æ­¥é•¿w)ï¼Œé»˜è®¤ä¸ºpool_size
    padding=â€˜validâ€™orâ€˜sameâ€™#ä½¿ç”¨å…¨é›¶å¡«å……æ˜¯â€œsameâ€ï¼Œä¸ä½¿ç”¨æ˜¯â€œvalidâ€ï¼ˆé»˜è®¤ï¼‰
)

# ä¾‹
model = tf.keras.models.Sequential([
    Conv2D(filters=6, kernel_size=(5, 5), padding='same'), # å·ç§¯å±‚
    BatchNormalization(), # BNå±‚
    Activation('relu'), # æ¿€æ´»å±‚
    MaxPool2D(pool_size=(2, 2), strides=2, padding='same'), # æ± åŒ–å±‚
    Dropout(0.2), # dropoutå±‚
])
```



## èˆå¼ƒ

> åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶ï¼Œå°†ä¸€éƒ¨åˆ†ç¥ç»å…ƒæŒ‰ç…§ä¸€å®šæ¦‚ç‡ä»ç¥ç»ç½‘ç»œä¸­æš‚æ—¶èˆå¼ƒã€‚ç¥ç»ç½‘ç»œä½¿ç”¨æ—¶ï¼Œè¢«èˆå¼ƒçš„ç¥ç»å…ƒä¹Ÿå°†æ¢å¤ã€‚

![image-20210914141813278](img/image-20210914141813278.png)



`tf.keras.layers.Dropout(èˆå¼ƒçš„æ¦‚ç‡)`

```python
model = tf.keras.models.Sequential([
    Conv2D(filters=6, kernel_size=(5, 5), padding='same'), # å·ç§¯å±‚
    BatchNormalization(), # BNå±‚
    Activation('relu'), # æ¿€æ´»å±‚
    MaxPool2D(pool_size=(2, 2), strides=2, padding='same'), # æ± åŒ–å±‚
    Dropout(0.2), # dropoutå±‚
])
```



## LeNet





## AlexNet







## ResNet







# å¾ªç¯ç¥ç»ç½‘ç»œ



































































